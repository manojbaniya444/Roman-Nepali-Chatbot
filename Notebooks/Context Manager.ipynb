{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOT8tTw2pDc4PjgDOwjR8s4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n","import re\n","from typing import Dict, List, Tuple, Optional"],"metadata":{"id":"m0NKaSim7hlq","executionInfo":{"status":"ok","timestamp":1735115355413,"user_tz":-345,"elapsed":462,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class NepaliEntityRecognition:\n","    def __init__(self):\n","        # Define entity types for e-commerce\n","        self.entity_types = {\n","            'PRODUCT': 0,\n","            'BRAND': 1,\n","            'PRICE': 2,\n","            'SIZE': 3,\n","            'COLOR': 4,\n","            'LOCATION': 5,\n","            'DATE': 6,\n","            'O': 7  # Other/Non-entity\n","        }\n","\n","        # Training data: (sentence, [(word, entity_type)])\n","        self.training_data = [\n","            (\"malai nike ko black shoes chaiyeko thiyo\", [\n","                (\"malai\", \"O\"),\n","                (\"nike\", \"BRAND\"),\n","                (\"ko\", \"O\"),\n","                (\"black\", \"COLOR\"),\n","                (\"shoes\", \"PRODUCT\"),\n","                (\"chaiyeko\", \"O\"),\n","                (\"thiyo\", \"O\")\n","            ]),\n","            (\"xs size ko red tshirt kati parcha\", [\n","                (\"xs\", \"SIZE\"),\n","                (\"size\", \"SIZE\"),\n","                (\"ko\", \"O\"),\n","                (\"red\", \"COLOR\"),\n","                (\"tshirt\", \"PRODUCT\"),\n","                (\"kati\", \"O\"),\n","                (\"parcha\", \"O\")\n","            ])\n","        ]\n","\n","        # Product catalog for entity validation\n","        self.catalog = {\n","            'products': ['shoes', 'tshirt', 'pants', 'jacket', 'bag'],\n","            'brands': ['nike', 'adidas', 'puma', 'reebok'],\n","            'colors': ['red', 'black', 'blue', 'white'],\n","            'sizes': ['xs', 's', 'm', 'l', 'xl']\n","        }\n","\n","    def build_model(self, vocab_size: int, max_length: int) -> Sequential:\n","        \"\"\"Build BiLSTM model for NER\"\"\"\n","        model = Sequential([\n","            Embedding(vocab_size, 64, input_length=max_length),\n","            Bidirectional(LSTM(32, return_sequences=True)),\n","            TimeDistributed(Dense(len(self.entity_types), activation='softmax'))\n","        ])\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","        return model\n","\n","    def extract_entities(self, text: str) -> List[Tuple[str, str]]:\n","        \"\"\"Extract entities from text using rules and catalog matching\"\"\"\n","        words = text.lower().split()\n","        entities = []\n","\n","        for word in words:\n","            if word in self.catalog['products']:\n","                entities.append((word, 'PRODUCT'))\n","            elif word in self.catalog['brands']:\n","                entities.append((word, 'BRAND'))\n","            elif word in self.catalog['colors']:\n","                entities.append((word, 'COLOR'))\n","            elif word in self.catalog['sizes']:\n","                entities.append((word, 'SIZE'))\n","            elif re.match(r'rs\\.?\\s*\\d+', word) or word.isdigit():\n","                entities.append((word, 'PRICE'))\n","            elif re.match(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', word):\n","                entities.append((word, 'DATE'))\n","            else:\n","                entities.append((word, 'O'))\n","\n","        return entities"],"metadata":{"id":"GWpcGYpx7l-5","executionInfo":{"status":"ok","timestamp":1735115359589,"user_tz":-345,"elapsed":439,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mVsACPxx7a1I","executionInfo":{"status":"ok","timestamp":1735115370445,"user_tz":-345,"elapsed":453,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"outputs":[],"source":["class ContextManager:\n","    def __init__(self):\n","        self.context = {}\n","        self.conversation_history = []\n","        self.current_intent = None\n","\n","    def update_context(self, user_id: str, entities: List[Tuple[str, str]], intent: str):\n","        \"\"\"Update context with new entities and intent\"\"\"\n","        if user_id not in self.context:\n","            self.context[user_id] = {\n","                'entities': {},\n","                'intent_history': [],\n","                'last_interaction': None\n","            }\n","\n","        # Update entities in context\n","        for entity, entity_type in entities:\n","            if entity_type != 'O':\n","                self.context[user_id]['entities'][entity_type] = entity\n","\n","        # Update intent history\n","        self.context[user_id]['intent_history'].append(intent)\n","        if len(self.context[user_id]['intent_history']) > 5:\n","            self.context[user_id]['intent_history'].pop(0)\n","\n","    def get_context(self, user_id: str) -> Dict:\n","        \"\"\"Get current context for user\"\"\"\n","        return self.context.get(user_id, {})\n","\n","    def clear_context(self, user_id: str):\n","        \"\"\"Clear context for user\"\"\"\n","        if user_id in self.context:\n","            del self.context[user_id]"]},{"cell_type":"code","source":["class EnhancedNepaliChatbot:\n","    def __init__(self):\n","        self.ner = NepaliEntityRecognition()\n","        self.context_manager = ContextManager()\n","\n","        # Enhanced response templates with entity placeholders\n","        self.response_templates = {\n","            'product_inquiry': [\n","                \"{COLOR} {PRODUCT} ko price Rs. {PRICE} cha\",\n","                \"{BRAND} ko {PRODUCT} ahile stock ma cha\",\n","                \"{SIZE} size ko {PRODUCT} available cha\"\n","            ],\n","            'delivery_inquiry': [\n","                \"{LOCATION} ma delivery 3-5 din lagcha\",\n","                \"tapai ko order {DATE} ma deliver huncha\"\n","            ]\n","        }\n","\n","    def process_query(self, user_id: str, query: str) -> str:\n","        \"\"\"Process user query with entity recognition and context\"\"\"\n","        # Extract entities\n","        entities = self.ner.extract_entities(query)\n","\n","        # Determine intent (simplified for example)\n","        intent = self._determine_intent(query)\n","\n","        # Update context\n","        self.context_manager.update_context(user_id, entities, intent)\n","\n","        # Generate response using context\n","        return self._generate_response(user_id, entities, intent)\n","\n","    def _determine_intent(self, query: str) -> str:\n","        \"\"\"Simplified intent determination\"\"\"\n","        if any(word in query.lower() for word in ['price', 'paisa', 'kati']):\n","            return 'product_inquiry'\n","        elif any(word in query.lower() for word in ['delivery', 'shipping']):\n","            return 'delivery_inquiry'\n","        return 'general_inquiry'\n","\n","    def _generate_response(self, user_id: str, entities: List[Tuple[str, str]], intent: str) -> str:\n","        \"\"\"Generate response using context and entities\"\"\"\n","        context = self.context_manager.get_context(user_id)\n","\n","        # Create entity dictionary from current entities\n","        entity_dict = {entity_type: entity for entity, entity_type in entities if entity_type != 'O'}\n","\n","        # Add missing entities from context\n","        for entity_type, entity in context.get('entities', {}).items():\n","            if entity_type not in entity_dict:\n","                entity_dict[entity_type] = entity\n","\n","        # Select response template\n","        if intent in self.response_templates:\n","            template = np.random.choice(self.response_templates[intent])\n","\n","            # Fill template with entities\n","            try:\n","                response = template.format(**entity_dict)\n","            except KeyError:\n","                response = \"Maaf garnuhos, thap jaankaari chaincha. Kripaya specific details dinuhos.\"\n","        else:\n","            response = \"Maaf garnuhos, ma tapai ko prashna bujhina.\"\n","\n","        return response"],"metadata":{"id":"znG6PRdr7tjd","executionInfo":{"status":"ok","timestamp":1735115376456,"user_tz":-345,"elapsed":433,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","def main():\n","    chatbot = EnhancedNepaliChatbot()\n","\n","    # Simulate conversation\n","    conversations = [\n","        (\"nike ko black shoes kati parcha?\", \"user123\"),\n","        (\"tyo shoes xs size ma cha?\", \"user123\"),\n","        (\"kathmandu ma delivery kati din lagcha?\", \"user123\")\n","    ]\n","\n","    for query, user_id in conversations:\n","        print(f\"\\nUser: {query}\")\n","        response = chatbot.process_query(user_id, query)\n","        print(f\"Bot: {response}\")\n","\n","        # Print current context\n","        print(\"\\nCurrent Context:\")\n","        print(chatbot.context_manager.get_context(user_id))"],"metadata":{"id":"jeLOEy_i7x42","executionInfo":{"status":"ok","timestamp":1735115392908,"user_tz":-345,"elapsed":416,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# testing\n","\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrWw3uH679Vs","executionInfo":{"status":"ok","timestamp":1735115409442,"user_tz":-345,"elapsed":645,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"07b17036-4268-4c31-a16e-b6fc38d5dad5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","User: nike ko black shoes kati parcha?\n","Bot: Maaf garnuhos, thap jaankaari chaincha. Kripaya specific details dinuhos.\n","\n","Current Context:\n","{'entities': {'BRAND': 'nike', 'COLOR': 'black', 'PRODUCT': 'shoes'}, 'intent_history': ['product_inquiry'], 'last_interaction': None}\n","\n","User: tyo shoes xs size ma cha?\n","Bot: Maaf garnuhos, ma tapai ko prashna bujhina.\n","\n","Current Context:\n","{'entities': {'BRAND': 'nike', 'COLOR': 'black', 'PRODUCT': 'shoes', 'SIZE': 'xs'}, 'intent_history': ['product_inquiry', 'general_inquiry'], 'last_interaction': None}\n","\n","User: kathmandu ma delivery kati din lagcha?\n","Bot: Maaf garnuhos, thap jaankaari chaincha. Kripaya specific details dinuhos.\n","\n","Current Context:\n","{'entities': {'BRAND': 'nike', 'COLOR': 'black', 'PRODUCT': 'shoes', 'SIZE': 'xs'}, 'intent_history': ['product_inquiry', 'general_inquiry', 'product_inquiry'], 'last_interaction': None}\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n","import re\n","from typing import Dict, List, Tuple, Optional\n","from datetime import datetime, timedelta\n","\n","class NepaliEntityRecognition:\n","    def __init__(self):\n","        self.entity_types = {\n","            'PRODUCT': 0,\n","            'BRAND': 1,\n","            'PRICE': 2,\n","            'SIZE': 3,\n","            'COLOR': 4,\n","            'LOCATION': 5,\n","            'DATE': 6,\n","            'O': 7\n","        }\n","\n","        # Extended product catalog with prices\n","        self.catalog = {\n","            'products': {\n","                'shoes': {'price_range': (2000, 15000)},\n","                'tshirt': {'price_range': (500, 3000)},\n","                'pants': {'price_range': (1000, 5000)},\n","                'jacket': {'price_range': (2500, 8000)},\n","                'bag': {'price_range': (1000, 6000)}\n","            },\n","            'brands': {\n","                'nike': {'premium': True},\n","                'adidas': {'premium': True},\n","                'puma': {'premium': False},\n","                'reebok': {'premium': False}\n","            },\n","            'colors': ['red', 'black', 'blue', 'white'],\n","            'sizes': ['xs', 's', 'm', 'l', 'xl'],\n","            'locations': {\n","                'kathmandu': {'delivery_days': 2},\n","                'lalitpur': {'delivery_days': 2},\n","                'bhaktapur': {'delivery_days': 2},\n","                'pokhara': {'delivery_days': 4},\n","                'biratnagar': {'delivery_days': 5}\n","            }\n","        }\n","\n","    def extract_entities(self, text: str) -> List[Tuple[str, str]]:\n","        words = text.lower().split()\n","        entities = []\n","\n","        for word in words:\n","            if word in self.catalog['products']:\n","                entities.append((word, 'PRODUCT'))\n","            elif word in self.catalog['brands']:\n","                entities.append((word, 'BRAND'))\n","            elif word in self.catalog['colors']:\n","                entities.append((word, 'COLOR'))\n","            elif word in self.catalog['sizes']:\n","                entities.append((word, 'SIZE'))\n","            elif word in self.catalog['locations']:\n","                entities.append((word, 'LOCATION'))\n","            elif re.match(r'rs\\.?\\s*\\d+', word) or word.isdigit():\n","                entities.append((word, 'PRICE'))\n","            elif re.match(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', word):\n","                entities.append((word, 'DATE'))\n","            else:\n","                entities.append((word, 'O'))\n","\n","        return entities\n","\n","    def get_product_price(self, product: str, brand: str) -> Tuple[int, int]:\n","        \"\"\"Get price range for a product based on brand\"\"\"\n","        base_range = self.catalog['products'].get(product, {'price_range': (0, 0)})['price_range']\n","        if brand in self.catalog['brands'] and self.catalog['brands'][brand]['premium']:\n","            return (int(base_range[0] * 1.5), int(base_range[1] * 1.5))\n","        return base_range\n","\n","class ContextManager:\n","    def __init__(self):\n","        self.context = {}\n","        self.conversation_history = []\n","        self.current_intent = None\n","\n","    def update_context(self, user_id: str, entities: List[Tuple[str, str]], intent: str):\n","        if user_id not in self.context:\n","            self.context[user_id] = {\n","                'entities': {},\n","                'intent_history': [],\n","                'last_interaction': datetime.now()\n","            }\n","\n","        # Update entities in context\n","        for entity, entity_type in entities:\n","            if entity_type != 'O':\n","                self.context[user_id]['entities'][entity_type] = entity\n","\n","        # Update intent history\n","        self.context[user_id]['intent_history'].append(intent)\n","        if len(self.context[user_id]['intent_history']) > 5:\n","            self.context[user_id]['intent_history'].pop(0)\n","\n","        self.context[user_id]['last_interaction'] = datetime.now()\n","\n","class EnhancedNepaliChatbot:\n","    def __init__(self):\n","        self.ner = NepaliEntityRecognition()\n","        self.context_manager = ContextManager()\n","\n","        # Enhanced response templates with more natural Romanized Nepali\n","        self.response_templates = {\n","            'product_inquiry': [\n","                \"{BRAND} ko {COLOR} {PRODUCT} ko price Rs. {MIN_PRICE} dekhi Rs. {MAX_PRICE} sama cha.\",\n","                \"{COLOR} {PRODUCT} ko price range Rs. {MIN_PRICE} - {MAX_PRICE} ho.\",\n","                \"{BRAND} ko {PRODUCT} ko price range Rs. {MIN_PRICE} - {MAX_PRICE} cha.\"\n","            ],\n","            'stock_inquiry': [\n","                \"{BRAND} ko {COLOR} {PRODUCT} {SIZE} size ma available cha.\",\n","                \"{SIZE} size ko {COLOR} {PRODUCT} stock ma cha.\",\n","                \"Hajur, {COLOR} {PRODUCT} {SIZE} size ma paunus.\"\n","            ],\n","            'delivery_inquiry': [\n","                \"{LOCATION} ma delivery {DAYS} din ma huncha.\",\n","                \"{LOCATION} ma parcel pugnalai {DAYS} din lagcha.\",\n","                \"Hajur ko {LOCATION} ko location ma delivery {DAYS} din bhitra huncha.\"\n","            ]\n","        }\n","\n","    def _determine_intent(self, query: str) -> str:\n","        \"\"\"Enhanced intent determination\"\"\"\n","        query = query.lower()\n","        if any(word in query for word in ['kati', 'price', 'paisa', 'rate']):\n","            return 'product_inquiry'\n","        elif any(word in query for word in ['size', 'available', 'cha', 'stock']):\n","            return 'stock_inquiry'\n","        elif any(word in query for word in ['delivery', 'shipping', 'pugihalcha', 'aipugcha']):\n","            return 'delivery_inquiry'\n","        return 'general_inquiry'\n","\n","    def _generate_response(self, user_id: str, entities: List[Tuple[str, str]], intent: str) -> str:\n","        context = self.context_manager.get_context(user_id)\n","        entity_dict = {entity_type: entity for entity, entity_type in entities if entity_type != 'O'}\n","\n","        # Add missing entities from context\n","        for entity_type, entity in context.get('entities', {}).items():\n","            if entity_type not in entity_dict:\n","                entity_dict[entity_type] = entity\n","\n","        # Generate appropriate response based on intent and available entities\n","        if intent == 'product_inquiry':\n","            if 'PRODUCT' in entity_dict:\n","                product = entity_dict.get('PRODUCT')\n","                brand = entity_dict.get('BRAND', 'standard')\n","                min_price, max_price = self.ner.get_product_price(product, brand)\n","                entity_dict['MIN_PRICE'] = min_price\n","                entity_dict['MAX_PRICE'] = max_price\n","                template = np.random.choice(self.response_templates['product_inquiry'])\n","                return template.format(**entity_dict)\n","\n","        elif intent == 'stock_inquiry':\n","            if all(key in entity_dict for key in ['PRODUCT', 'SIZE']):\n","                template = np.random.choice(self.response_templates['stock_inquiry'])\n","                return template.format(**entity_dict)\n","\n","        elif intent == 'delivery_inquiry':\n","            if 'LOCATION' in entity_dict:\n","                location = entity_dict['LOCATION']\n","                days = self.ner.catalog['locations'].get(location, {'delivery_days': 5})['delivery_days']\n","                entity_dict['DAYS'] = days\n","                template = np.random.choice(self.response_templates['delivery_inquiry'])\n","                return template.format(**entity_dict)\n","\n","        # If we can't generate a specific response, ask for more details\n","        missing_info = self._get_missing_info(intent, entity_dict)\n","        return f\"Kripaya {missing_info} specify garnuhos.\"\n","\n","    def _get_missing_info(self, intent: str, entities: Dict) -> str:\n","        \"\"\"Determine what information is missing for a complete response\"\"\"\n","        if intent == 'product_inquiry':\n","            if 'PRODUCT' not in entities:\n","                return \"kun product\"\n","            if 'BRAND' not in entities:\n","                return \"kun brand\"\n","        elif intent == 'stock_inquiry':\n","            if 'PRODUCT' not in entities:\n","                return \"kun product\"\n","            if 'SIZE' not in entities:\n","                return \"kun size\"\n","        elif intent == 'delivery_inquiry':\n","            if 'LOCATION' not in entities:\n","                return \"delivery location\"\n","        return \"thap details\"\n","\n","    def process_query(self, user_id: str, query: str) -> str:\n","        entities = self.ner.extract_entities(query)\n","        intent = self._determine_intent(query)\n","        self.context_manager.update_context(user_id, entities, intent)\n","        return self._generate_response(user_id, entities, intent)\n","\n","# Example usage with the same queries\n","def main():\n","    chatbot = EnhancedNepaliChatbot()\n","\n","    conversations = [\n","        (\"nike ko black shoes kati parcha?\", \"user123\"),\n","        (\"tyo shoes xs size ma cha?\", \"user123\"),\n","        (\"kathmandu ma delivery kati din lagcha?\", \"user123\")\n","    ]\n","\n","    for query, user_id in conversations:\n","        print(f\"\\nUser: {query}\")\n","        response = chatbot.process_query(user_id, query)\n","        print(f\"Bot: {response}\")\n","        print(\"\\nCurrent Context:\")\n","        print(chatbot.context_manager.get_context(user_id))\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"-6Pq6pKM8jwo","executionInfo":{"status":"error","timestamp":1735115555989,"user_tz":-345,"elapsed":456,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"57724fc4-d2a9-4169-cc6e-90cb01e736c6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","User: nike ko black shoes kati parcha?\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'ContextManager' object has no attribute 'get_context'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-efcf422bb067>\u001b[0m in \u001b[0;36m<cell line: 217>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-efcf422bb067>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nUser: {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bot: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCurrent Context:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-efcf422bb067>\u001b[0m in \u001b[0;36mprocess_query\u001b[0;34m(self, user_id, query)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mintent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_intent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m# Example usage with the same queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-efcf422bb067>\u001b[0m in \u001b[0;36m_generate_response\u001b[0;34m(self, user_id, entities, intent)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mentity_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mentity_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentity_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ContextManager' object has no attribute 'get_context'"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import json\n","\n","class NepaliChatbotTrainer:\n","    def __init__(self, max_words=5000, max_len=50):\n","        self.max_words = max_words\n","        self.max_len = max_len\n","        self.tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n","\n","        # Training data for intent classification\n","        self.intent_data = {\n","            \"product_inquiry\": [\n","                \"yo product ko price kati ho\",\n","                \"kati parcha\",\n","                \"rate kati cha\",\n","                \"price kati ho\",\n","                \"yesko dam kati ho\",\n","                \"nike ko shoes kati parcha\",\n","                \"tshirt ko price kati ho\",\n","                \"jacket ko rate bhannus na\",\n","                \"pants ko price thaha cha\"\n","            ],\n","            \"stock_inquiry\": [\n","                \"stock ma cha\",\n","                \"available cha\",\n","                \"xs size cha\",\n","                \"large size paucha\",\n","                \"red color cha\",\n","                \"black shoes cha\",\n","                \"blue tshirt available cha\",\n","                \"nike ko shoes stock ma cha\"\n","            ],\n","            \"delivery_inquiry\": [\n","                \"delivery kati din ma huncha\",\n","                \"shipping time kati ho\",\n","                \"kahile samma aipugcha\",\n","                \"kathmandu ma kati din ma aaucha\",\n","                \"delivery charge kati ho\",\n","                \"shipping cost kati parcha\",\n","                \"delivery free cha\"\n","            ]\n","        }\n","\n","        # Training data for NER\n","        self.ner_data = [\n","            (\"nike ko black shoes kati parcha\", [\n","                (\"nike\", \"BRAND\"),\n","                (\"ko\", \"O\"),\n","                (\"black\", \"COLOR\"),\n","                (\"shoes\", \"PRODUCT\"),\n","                (\"kati\", \"O\"),\n","                (\"parcha\", \"O\")\n","            ]),\n","            (\"red tshirt xs size ma cha\", [\n","                (\"red\", \"COLOR\"),\n","                (\"tshirt\", \"PRODUCT\"),\n","                (\"xs\", \"SIZE\"),\n","                (\"size\", \"O\"),\n","                (\"ma\", \"O\"),\n","                (\"cha\", \"O\")\n","            ]),\n","            (\"kathmandu ma delivery kati din lagcha\", [\n","                (\"kathmandu\", \"LOCATION\"),\n","                (\"ma\", \"O\"),\n","                (\"delivery\", \"O\"),\n","                (\"kati\", \"O\"),\n","                (\"din\", \"O\"),\n","                (\"lagcha\", \"O\")\n","            ])\n","        ]\n","\n","    def prepare_intent_data(self):\n","        \"\"\"Prepare data for intent classification training\"\"\"\n","        texts = []\n","        labels = []\n","        label_to_id = {}\n","\n","        for idx, (intent, sentences) in enumerate(self.intent_data.items()):\n","            label_to_id[intent] = idx\n","            for sentence in sentences:\n","                texts.append(sentence)\n","                labels.append(idx)\n","\n","        # Tokenize texts\n","        self.tokenizer.fit_on_texts(texts)\n","        sequences = self.tokenizer.texts_to_sequences(texts)\n","        X = pad_sequences(sequences, maxlen=self.max_len)\n","        y = to_categorical(labels)\n","\n","        return X, y, label_to_id\n","\n","    def prepare_ner_data(self):\n","        \"\"\"Prepare data for NER training\"\"\"\n","        texts = []\n","        labels = []\n","\n","        # Create word and tag vocabularies\n","        words = set()\n","        tags = set()\n","\n","        for sentence, annotations in self.ner_data:\n","            words.update([word for word, _ in annotations])\n","            tags.update([tag for _, tag in annotations])\n","\n","        word_to_id = {word: idx for idx, word in enumerate(words, 1)}\n","        tag_to_id = {tag: idx for idx, tag in enumerate(tags)}\n","\n","        # Convert to sequences\n","        for sentence, annotations in self.ner_data:\n","            text_seq = [word_to_id.get(word, 0) for word, _ in annotations]\n","            label_seq = [tag_to_id[tag] for _, tag in annotations]\n","\n","            texts.append(text_seq)\n","            labels.append(label_seq)\n","\n","        # Pad sequences\n","        X = pad_sequences(texts, maxlen=self.max_len, padding='post')\n","        y = pad_sequences(labels, maxlen=self.max_len, padding='post')\n","        y = to_categorical(y, num_classes=len(tag_to_id))\n","\n","        return X, y, word_to_id, tag_to_id\n","\n","    def build_intent_model(self, num_classes):\n","        \"\"\"Build intent classification model\"\"\"\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Embedding(self.max_words, 128, input_length=self.max_len),\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","            tf.keras.layers.Dense(64, activation='relu'),\n","            tf.keras.layers.Dropout(0.5),\n","            tf.keras.layers.Dense(num_classes, activation='softmax')\n","        ])\n","\n","        model.compile(optimizer='adam',\n","                     loss='categorical_crossentropy',\n","                     metrics=['accuracy'])\n","        return model\n","\n","    def build_ner_model(self, vocab_size, num_tags):\n","        \"\"\"Build NER model\"\"\"\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Embedding(vocab_size + 1, 128, input_length=self.max_len),\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n","            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='relu')),\n","            tf.keras.layers.Dropout(0.5),\n","            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags, activation='softmax'))\n","        ])\n","\n","        model.compile(optimizer='adam',\n","                     loss='categorical_crossentropy',\n","                     metrics=['accuracy'])\n","        return model\n","\n","    def train_models(self):\n","        \"\"\"Train both intent and NER models\"\"\"\n","        # Train intent classification model\n","        X_intent, y_intent, intent_label_map = self.prepare_intent_data()\n","        X_train_intent, X_test_intent, y_train_intent, y_test_intent = train_test_split(\n","            X_intent, y_intent, test_size=0.2, random_state=42\n","        )\n","\n","        intent_model = self.build_intent_model(len(intent_label_map))\n","        intent_history = intent_model.fit(\n","            X_train_intent, y_train_intent,\n","            epochs=10,\n","            batch_size=32,\n","            validation_data=(X_test_intent, y_test_intent),\n","            verbose=1\n","        )\n","\n","        # Train NER model\n","        X_ner, y_ner, word_map, tag_map = self.prepare_ner_data()\n","        X_train_ner, X_test_ner, y_train_ner, y_test_ner = train_test_split(\n","            X_ner, y_ner, test_size=0.2, random_state=42\n","        )\n","\n","        ner_model = self.build_ner_model(len(word_map), len(tag_map))\n","        ner_history = ner_model.fit(\n","            X_train_ner, y_train_ner,\n","            epochs=10,\n","            batch_size=32,\n","            validation_data=(X_test_ner, y_test_ner),\n","            verbose=1\n","        )\n","\n","        # Save models and mappings\n","        self.save_models(intent_model, ner_model,\n","                        intent_label_map, word_map, tag_map)\n","\n","        return intent_history, ner_history\n","\n","    def save_models(self, intent_model, ner_model,\n","                   intent_label_map, word_map, tag_map):\n","        \"\"\"Save trained models and mappings\"\"\"\n","        # Save models\n","        intent_model.save('intent_model.h5')\n","        ner_model.save('ner_model.h5')\n","\n","        # Save tokenizer\n","        with open('tokenizer.pickle', 'wb') as handle:\n","            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        # Save mappings\n","        mappings = {\n","            'intent_label_map': intent_label_map,\n","            'word_map': word_map,\n","            'tag_map': tag_map\n","        }\n","        with open('mappings.json', 'w', encoding='utf-8') as f:\n","            json.dump(mappings, f, ensure_ascii=False, indent=2)\n","\n","def main():\n","    # Initialize and train the models\n","    trainer = NepaliChatbotTrainer()\n","    intent_history, ner_history = trainer.train_models()\n","\n","    # Print training results\n","    print(\"\\nIntent Classification Training Results:\")\n","    print(f\"Final accuracy: {intent_history.history['accuracy'][-1]:.4f}\")\n","    print(f\"Final validation accuracy: {intent_history.history['val_accuracy'][-1]:.4f}\")\n","\n","    print(\"\\nNER Training Results:\")\n","    print(f\"Final accuracy: {ner_history.history['accuracy'][-1]:.4f}\")\n","    print(f\"Final validation accuracy: {ner_history.history['val_accuracy'][-1]:.4f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPR6wTMe89k2","executionInfo":{"status":"ok","timestamp":1735115688126,"user_tz":-345,"elapsed":25966,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"2939ff12-408f-4e99-d1ab-a5b48539279c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.3684 - loss: 1.0988 - val_accuracy: 0.4000 - val_loss: 1.0889\n","Epoch 2/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step - accuracy: 0.3684 - loss: 1.0889 - val_accuracy: 0.4000 - val_loss: 1.0846\n","Epoch 3/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.3684 - loss: 1.0928 - val_accuracy: 0.4000 - val_loss: 1.0795\n","Epoch 4/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.4211 - loss: 1.0737 - val_accuracy: 0.4000 - val_loss: 1.0723\n","Epoch 5/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.4211 - loss: 1.0696 - val_accuracy: 0.4000 - val_loss: 1.0671\n","Epoch 6/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.3684 - loss: 1.0863 - val_accuracy: 0.4000 - val_loss: 1.0640\n","Epoch 7/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.3684 - loss: 1.0682 - val_accuracy: 0.4000 - val_loss: 1.0611\n","Epoch 8/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.4737 - loss: 1.0704 - val_accuracy: 0.4000 - val_loss: 1.0601\n","Epoch 9/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.4737 - loss: 1.0510 - val_accuracy: 0.4000 - val_loss: 1.0582\n","Epoch 10/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.3684 - loss: 1.0697 - val_accuracy: 0.4000 - val_loss: 1.0537\n","Epoch 1/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 0.3800 - loss: 1.7706 - val_accuracy: 0.9400 - val_loss: 1.6804\n","Epoch 2/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7800 - loss: 1.6864 - val_accuracy: 0.9400 - val_loss: 1.5815\n","Epoch 3/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9500 - loss: 1.5897 - val_accuracy: 0.9400 - val_loss: 1.4601\n","Epoch 4/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9600 - loss: 1.4561 - val_accuracy: 0.9400 - val_loss: 1.3104\n","Epoch 5/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9500 - loss: 1.3159 - val_accuracy: 0.9400 - val_loss: 1.1327\n","Epoch 6/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9500 - loss: 1.1650 - val_accuracy: 0.9400 - val_loss: 0.9295\n","Epoch 7/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9600 - loss: 0.9402 - val_accuracy: 0.9400 - val_loss: 0.7137\n","Epoch 8/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9400 - loss: 0.6996 - val_accuracy: 0.9400 - val_loss: 0.5137\n","Epoch 9/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9500 - loss: 0.5862 - val_accuracy: 0.9400 - val_loss: 0.3605\n","Epoch 10/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9600 - loss: 0.3822 - val_accuracy: 0.9400 - val_loss: 0.2661\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\n","Intent Classification Training Results:\n","Final accuracy: 0.3684\n","Final validation accuracy: 0.4000\n","\n","NER Training Results:\n","Final accuracy: 0.9600\n","Final validation accuracy: 0.9400\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import json\n","import pickle\n","\n","class ChatbotTester:\n","    def __init__(self):\n","        # Load trained models\n","        try:\n","            self.intent_model = tf.keras.models.load_model('intent_model.h5')\n","            self.ner_model = tf.keras.models.load_model('ner_model.h5')\n","\n","            # Load tokenizer\n","            with open('tokenizer.pickle', 'rb') as handle:\n","                self.tokenizer = pickle.load(handle)\n","\n","            # Load mappings\n","            with open('mappings.json', 'r', encoding='utf-8') as f:\n","                self.mappings = json.load(f)\n","\n","            self.id_to_intent = {v: k for k, v in self.mappings['intent_label_map'].items()}\n","            self.id_to_tag = {v: k for k, v in self.mappings['tag_map'].items()}\n","\n","            print(\"Models and mappings loaded successfully!\")\n","\n","        except Exception as e:\n","            print(f\"Error loading models: {e}\")\n","            print(\"\\nFirst run the training script to generate the models!\")\n","            return\n","\n","        # Test cases\n","        self.test_cases = [\n","            # Product inquiries\n","            \"nike ko black shoes kati parcha\",\n","            \"red tshirt ko price kati ho\",\n","            \"adidas ko jacket ko rate bhannus na\",\n","            \"xs size ko pant kati parcha\",\n","\n","            # Stock inquiries\n","            \"black nike shoes xs size ma cha\",\n","            \"l size ko red tshirt available cha\",\n","            \"white adidas shoes stock ma cha\",\n","            \"blue color ko jacket paucha\",\n","\n","            # Delivery inquiries\n","            \"kathmandu ma delivery kati din ma huncha\",\n","            \"pokhara ma shipping kati din lagcha\",\n","            \"delhi ma pugihalcha\",\n","            \"shipping charge kati ho\",\n","\n","            # Mixed queries\n","            \"nike ko black shoes xs size ma cha ani price kati ho\",\n","            \"kathmandu ma delivery free cha? ani kati din ma aaucha?\",\n","\n","            # Edge cases\n","            \"k cha\",\n","            \"namaste\",\n","            \"dhanyabaad\",\n","            \"ma kinchu\"\n","        ]\n","\n","    def predict_intent(self, text):\n","        \"\"\"Predict intent for given text\"\"\"\n","        # Tokenize and pad the text\n","        sequence = self.tokenizer.texts_to_sequences([text])\n","        padded = pad_sequences(sequence, maxlen=50)\n","\n","        # Predict intent\n","        prediction = self.intent_model.predict(padded)[0]\n","        intent_id = np.argmax(prediction)\n","        confidence = prediction[intent_id]\n","\n","        return self.id_to_intent[intent_id], confidence\n","\n","    def predict_entities(self, text):\n","        \"\"\"Predict entities in given text\"\"\"\n","        # Convert text to sequence using word_map\n","        words = text.split()\n","        word_ids = [self.mappings['word_map'].get(word, 0) for word in words]\n","        padded = pad_sequences([word_ids], maxlen=50, padding='post')\n","\n","        # Predict entities\n","        predictions = self.ner_model.predict(padded)[0]\n","\n","        entities = []\n","        for i, word in enumerate(words):\n","            if i < len(predictions):\n","                tag_id = np.argmax(predictions[i])\n","                confidence = predictions[i][tag_id]\n","                if tag_id in self.id_to_tag:\n","                    tag = self.id_to_tag[tag_id]\n","                    if tag != 'O':  # Only include named entities\n","                        entities.append({\n","                            'word': word,\n","                            'entity': tag,\n","                            'confidence': float(confidence)\n","                        })\n","\n","        return entities\n","\n","    def run_tests(self):\n","        \"\"\"Run tests on all test cases\"\"\"\n","        print(\"\\nRunning tests...\\n\")\n","        print(\"=\" * 80)\n","\n","        for i, test_case in enumerate(self.test_cases, 1):\n","            print(f\"\\nTest Case {i}: '{test_case}'\")\n","            print(\"-\" * 40)\n","\n","            # Predict intent\n","            intent, confidence = self.predict_intent(test_case)\n","            print(f\"Intent: {intent} (confidence: {confidence:.4f})\")\n","\n","            # Predict entities\n","            entities = self.predict_entities(test_case)\n","            if entities:\n","                print(\"\\nEntities found:\")\n","                for entity in entities:\n","                    print(f\"- {entity['word']}: {entity['entity']} \"\n","                          f\"(confidence: {entity['confidence']:.4f})\")\n","            else:\n","                print(\"\\nNo entities found\")\n","\n","            print(\"-\" * 40)\n","\n","        print(\"\\nTest completion summary:\")\n","        print(\"=\" * 80)\n","\n","    def interactive_test(self):\n","        \"\"\"Interactive testing mode\"\"\"\n","        print(\"\\nEntering interactive testing mode (type 'exit' to quit)\")\n","        print(\"=\" * 80)\n","\n","        while True:\n","            query = input(\"\\nEnter your query: \")\n","            if query.lower() == 'exit':\n","                break\n","\n","            print(\"\\nAnalyzing query...\")\n","            print(\"-\" * 40)\n","\n","            # Predict intent\n","            intent, confidence = self.predict_intent(query)\n","            print(f\"Intent: {intent} (confidence: {confidence:.4f})\")\n","\n","            # Predict entities\n","            entities = self.predict_entities(query)\n","            if entities:\n","                print(\"\\nEntities found:\")\n","                for entity in entities:\n","                    print(f\"- {entity['word']}: {entity['entity']} \"\n","                          f\"(confidence: {entity['confidence']:.4f})\")\n","            else:\n","                print(\"\\nNo entities found\")\n","\n","            print(\"-\" * 40)\n","\n","def main():\n","    tester = ChatbotTester()\n","\n","    # Run automated tests\n","    tester.run_tests()\n","\n","    # Start interactive testing\n","    print(\"\\nWould you like to try interactive testing? (yes/no)\")\n","    response = input()\n","    if response.lower() in ['yes', 'y']:\n","        tester.interactive_test()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUniKQOI9Rgp","executionInfo":{"status":"ok","timestamp":1735115783890,"user_tz":-345,"elapsed":39982,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"3d7cb572-21bc-40e4-f31c-f0f116ad9d9e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["Models and mappings loaded successfully!\n","\n","Running tests...\n","\n","================================================================================\n","\n","Test Case 1: 'nike ko black shoes kati parcha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step\n","Intent: product_inquiry (confidence: 0.3989)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 2: 'red tshirt ko price kati ho'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","Intent: product_inquiry (confidence: 0.4112)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 3: 'adidas ko jacket ko rate bhannus na'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","Intent: product_inquiry (confidence: 0.4037)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 4: 'xs size ko pant kati parcha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","Intent: product_inquiry (confidence: 0.3975)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 5: 'black nike shoes xs size ma cha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Intent: product_inquiry (confidence: 0.3651)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 6: 'l size ko red tshirt available cha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","Intent: product_inquiry (confidence: 0.3657)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 7: 'white adidas shoes stock ma cha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","Intent: product_inquiry (confidence: 0.3645)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 8: 'blue color ko jacket paucha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","Intent: product_inquiry (confidence: 0.3836)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 9: 'kathmandu ma delivery kati din ma huncha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","Intent: product_inquiry (confidence: 0.3666)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 10: 'pokhara ma shipping kati din lagcha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","Intent: product_inquiry (confidence: 0.3808)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 11: 'delhi ma pugihalcha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","Intent: product_inquiry (confidence: 0.3734)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 12: 'shipping charge kati ho'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Intent: product_inquiry (confidence: 0.3996)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 13: 'nike ko black shoes xs size ma cha ani price kati ho'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","Intent: product_inquiry (confidence: 0.4021)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 14: 'kathmandu ma delivery free cha? ani kati din ma aaucha?'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","Intent: product_inquiry (confidence: 0.3646)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 15: 'k cha'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","Intent: product_inquiry (confidence: 0.3687)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 16: 'namaste'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Intent: product_inquiry (confidence: 0.3775)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 17: 'dhanyabaad'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","Intent: product_inquiry (confidence: 0.3775)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test Case 18: 'ma kinchu'\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Intent: product_inquiry (confidence: 0.3731)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Test completion summary:\n","================================================================================\n","\n","Would you like to try interactive testing? (yes/no)\n","yes\n","\n","Entering interactive testing mode (type 'exit' to quit)\n","================================================================================\n","\n","Enter your query: Store location kaha xa\n","\n","Analyzing query...\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","Intent: product_inquiry (confidence: 0.3792)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Enter your query: Adidas ko tshirt kati ho\n","\n","Analyzing query...\n","----------------------------------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","Intent: product_inquiry (confidence: 0.4079)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\n","No entities found\n","----------------------------------------\n","\n","Enter your query: exit\n"]}]}]}