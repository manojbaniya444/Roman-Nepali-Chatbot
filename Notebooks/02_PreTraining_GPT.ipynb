{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## PreTraining on Unlabeled Data the GPT Model"],"metadata":{"id":"CaXhEikphfYf"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt"],"metadata":{"id":"UKUqDLv7jyfQ","executionInfo":{"status":"ok","timestamp":1734934874368,"user_tz":-345,"elapsed":576,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","\n","tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n","\n","from tokenizers.trainers import BpeTrainer\n","\n","trainer = BpeTrainer(\n","    special_tokens=[\"[UNK]\", \"[PAD]\", \"[MASK]\", \"<|endoftext|>\"]\n",")"],"metadata":{"id":"JRebGlbWB1QX","executionInfo":{"status":"ok","timestamp":1734934875074,"user_tz":-345,"elapsed":1,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["from tokenizers.pre_tokenizers import Whitespace\n","\n","tokenizer.pre_tokenizer = Whitespace()"],"metadata":{"id":"ql-YRpAEB7Yg","executionInfo":{"status":"ok","timestamp":1734934875075,"user_tz":-345,"elapsed":2,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["tokenizer.train([\"/content/roman_02.txt\"], trainer)"],"metadata":{"id":"tHNlbtHUB9rP","executionInfo":{"status":"ok","timestamp":1734934934842,"user_tz":-345,"elapsed":59019,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["tokenizer.save(\"tokenizer_roman_02.json\")"],"metadata":{"id":"h2Q9EyY9CTeA","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":6,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["vocab_size = tokenizer.get_vocab_size()"],"metadata":{"id":"VS6kifJ3Ks4F","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["class LayerNormalization(torch.nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False) # bessel correction n instead of (n -1 )\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps) # prevent divide by zero\n","        return norm_x * self.scale + self.shift # two learnable parameters model learns during training"],"metadata":{"id":"y817pGRSkKv6","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["class GeLU(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))"],"metadata":{"id":"5TuhTC-PkLbn","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["class FeedForward(torch.nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"embedding_size\"], 4 * cfg[\"embedding_size\"]),\n","            GeLU(),\n","            nn.Linear(4 * cfg[\"embedding_size\"], cfg[\"embedding_size\"])\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"744wovlHkQ2U","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads\n","        self.W_Query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_Key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_Value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.size() # batch, num_tokens, dimension\n","\n","        keys = self.W_Key(x)\n","        queries = self.W_Query(x)\n","        values = self.W_Value(x)\n","        # We change last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim) to split the d_out in num_heads part\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # transpose (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        attention_scores = queries @ keys.transpose(-2, -1) # we transpose the last two dimension as the first two will be broadcased\n","\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","        attention_scores.masked_fill_(mask_bool, float(\"-inf\"))\n","\n","        attention_weights = torch.nn.functional.softmax(attention_scores / keys.shape[-1] ** 0.5, dim=-1)\n","\n","        attention_weights = self.dropout(attention_weights)\n","\n","        context_vectors = (attention_weights @ values).transpose(1, 2) # (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n","\n","        context_vectors = context_vectors.contiguous().view(b, num_tokens, self.d_out)\n","        context_vectors = self.out_proj(context_vectors)\n","\n","        return context_vectors"],"metadata":{"id":"xqg0OQ5nkVVK","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"embedding_size\"],\n","            d_out=cfg[\"embedding_size\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"]\n","        )\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNormalization(cfg[\"embedding_size\"])\n","        self.norm2 = LayerNormalization(cfg[\"embedding_size\"])\n","        self.drop_skip = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        skip = x\n","        x = self.norm1(x)\n","        x = self.att(x)\n","        x = self.drop_skip(x)\n","        x = x + skip\n","\n","        skip = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_skip(x)\n","        x = x + skip\n","\n","        return x"],"metadata":{"id":"GXK1tmUqkXNS","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_size\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_size\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n","        )\n","\n","        self.final_norm = LayerNormalization(cfg[\"embedding_size\"])\n","        self.out_head = nn.Linear(cfg[\"embedding_size\"], cfg[\"vocab_size\"], bias=False)\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","\n","        return logits"],"metadata":{"id":"v31M3jf8kaw-","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","execution_count":84,"metadata":{"id":"6v8u77qchQ4q","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"outputs":[],"source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": tokenizer.get_vocab_size(),\n","    \"context_length\": 512,\n","    \"embedding_size\": 512,\n","    \"n_heads\": 8,\n","    \"n_layers\": 4,\n","    \"drop_rate\": 0.1,\n","    \"qkv_bias\": False\n","}"]},{"cell_type":"code","source":["model = GPTModel(GPT_CONFIG_124M)"],"metadata":{"id":"eVDOddtPmN7R","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["## Utils Function"],"metadata":{"id":"jqc13OKrlNll"}},{"cell_type":"code","source":["def generate_text_sample(model, idx, max_new_tokens, context_size):\n","    \"\"\"\n","    Generates a new token in autoregressive manner given a seed token\n","\n","    Args:\n","       - model: The GPT Model\n","       - idx: The seed text token ids\n","       - max_new_tokens: The maximum number of tokens we want to generate\n","       - context_size: The number of tokens to consider at one time during inference\n","    \"\"\"\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:] # get all batch, and only the last context_size tokens\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        logits = logits[:, -1, :] # get the logits for only the last token which is what we used to predict the next token\n","        probs = torch.softmax(logits, dim=-1)\n","        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n","        idx = torch.cat([idx, idx_next], dim=1)\n","\n","    return idx"],"metadata":{"id":"3WqbDGE0lPtJ","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["# def text_to_token_ids(text, tokenizer):\n","#   encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","#   encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","#   return encoded_tensor\n","\n","# def token_ids_to_text(token_ids, tokenizer):\n","#   flat = token_ids.squeeze(0)\n","#   return tokenizer.decode(flat.tolist())"],"metadata":{"id":"PnWp9H7MlUsc","executionInfo":{"status":"ok","timestamp":1734934934843,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["def text_to_token_ids(text, tokenizer):\n","  encoded = tokenizer.encode(text).ids\n","  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","  return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","  flat = token_ids.squeeze(0)\n","  return tokenizer.decode(flat.tolist())"],"metadata":{"id":"SIkVt_j5CyVD","executionInfo":{"status":"ok","timestamp":1734934935740,"user_tz":-345,"elapsed":900,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["start_context = \"Nepal\""],"metadata":{"id":"3Tam4OMFCpSe","executionInfo":{"status":"ok","timestamp":1734934935740,"user_tz":-345,"elapsed":22,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["token_ids = generate_text_sample(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNBCt5UGl1aI","executionInfo":{"status":"ok","timestamp":1734934935740,"user_tz":-345,"elapsed":21,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"a211d678-ed6e-46e5-dccf-0a92d2dd0c31"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["e pal sthaniyavasilai kalpanika kshamatama yatharthama tithima dekh sambandhana talabako sukhi aphusa\n"]}]},{"cell_type":"markdown","source":["## Model Generalization Error"],"metadata":{"id":"pYO6rZWtmfgD"}},{"cell_type":"code","source":["inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n","                       [40,    1107, 588]])   #  \"I really like\"]\n","\n","targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n","                        [1107,  588, 11311]]) #  \" really like chocolate\"]"],"metadata":{"id":"re3k4pyWmhzq","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":22,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  logits = model(inputs)\n","\n","\n","probabilities = torch.softmax(logits, dim=-1)\n","print(probabilities.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3jruZ21Y3F1","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":21,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"9dc73ce0-e411-47c4-80d7-90e8cc5023df"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 30000])\n"]}]},{"cell_type":"code","source":["token_ids = torch.argmax(probabilities, dim=-1, keepdim=True)\n","print(f\"Token Ids: {token_ids}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cY2a5qCLZHv0","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":20,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"e42ebf2b-e554-4c52-8430-c9bdcd6a724f"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Token Ids: tensor([[[ 6510],\n","         [24956],\n","         [27107]],\n","\n","        [[21884],\n","         [13735],\n","         [19380]]])\n"]}]},{"cell_type":"code","source":["print(f\"Targets of 1st batch: {token_ids_to_text(targets[0], tokenizer)}\")\n","print(f\"Outputs of 1st batch: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkW1p_1nZZXU","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":19,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"6ac5f096-e55c-4852-86c0-222f3ecfad60"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["Targets of 1st batch: rgha chaitra â€˜\n","Outputs of 1st batch: parikshama ashramama bayometrika\n"]}]},{"cell_type":"code","source":["text_idx = 0\n","target_probs_1 = probabilities[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 1:\", target_probs_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQpvxhu7asib","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":18,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"8cd526b1-b5d0-48bc-b43c-d02a3f8a41f3"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["Text 1: tensor([4.6326e-05, 1.4399e-05, 2.5364e-05])\n"]}]},{"cell_type":"code","source":["text_idx = 1\n","target_probas_2 = probabilities[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 2:\", target_probas_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpB20K6na2fJ","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":16,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"1e4cdc8a-c6b3-4a23-9069-eeef1a791737"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Text 2: tensor([2.2811e-05, 6.6859e-05, 6.7203e-05])\n"]}]},{"cell_type":"markdown","source":["### Log Likelihood"],"metadata":{"id":"oNv-3pLEgT4e"}},{"cell_type":"markdown","source":["The goal of the training is to maximize these probabilities and get as close to 1."],"metadata":{"id":"ueA_8d5ZbFqn"}},{"cell_type":"code","source":["# Compute logarithm of all token probabilities\n","log_probas = torch.log(torch.cat((target_probs_1, target_probas_2)))\n","print(log_probas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYokOUlzbNLl","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":16,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"f6b73293-6760-4ab5-c67f-078cfdbf7c5d"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ -9.9798, -11.1484, -10.5822, -10.6883,  -9.6129,  -9.6078])\n"]}]},{"cell_type":"code","source":["torch.log(torch.tensor(0.01))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4ApcW6kdiM5","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":15,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"22043094-4c9b-4ba0-d1d1-bbcbb4f56a12"},"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-4.6052)"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["average_log_probabilities = torch.mean(log_probas)\n","print(average_log_probabilities) # calculation of average log likelihood"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbnc8thAcHL9","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":13,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"fa21df93-fe14-4b7e-9d26-90d8b21dcc7c"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-10.2699)\n"]}]},{"cell_type":"code","source":["# negative log likelihood\n","negative_log_likelihood = -1 * average_log_probabilities\n","print(negative_log_likelihood)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9ZHfulHcN7N","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":12,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"fb7d6a90-7391-4dea-e929-3285945531ef"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.2699)\n"]}]},{"cell_type":"markdown","source":["The common practice is to minimize the negative log likelihood as close to 0"],"metadata":{"id":"2PSbgO12d09q"}},{"cell_type":"code","source":["print(f\"Logits shape: {logits.shape}\")\n","print(f\"Targets shape: {targets.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMZVjayVd8Xv","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":11,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"aa4dee7c-4751-4f72-b594-963414a41bbc"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Logits shape: torch.Size([2, 3, 30000])\n","Targets shape: torch.Size([2, 3])\n"]}]},{"cell_type":"code","source":["tensor_a = torch.tensor([[[1,2,3],\n","                          [4,5,6]],\n","                         [[9,8,9],\n","                         [2,1,4]]])\n","tensor_b = torch.tensor([[900,800],\n","                         [200,400]])\n","\n","tensor_a.shape, tensor_b.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lruaE2nfVAl","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":10,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"d0f0eeb2-b586-4d27-d68f-69d4537a6908"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([2, 2, 3]), torch.Size([2, 2]))"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["tensor_a_flat = tensor_a.flatten(start_dim=0, end_dim=1)\n","tensor_b_flat = tensor_b.flatten(0, 1)\n","\n","tensor_a, tensor_a_flat, tensor_b, tensor_b_flat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WF3dNyQfy22","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":9,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"025e1cb8-3120-4d5f-e0c7-e2aa640813d8"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[1, 2, 3],\n","          [4, 5, 6]],\n"," \n","         [[9, 8, 9],\n","          [2, 1, 4]]]),\n"," tensor([[1, 2, 3],\n","         [4, 5, 6],\n","         [9, 8, 9],\n","         [2, 1, 4]]),\n"," tensor([[900, 800],\n","         [200, 400]]),\n"," tensor([900, 800, 200, 400]))"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["logits_flat = logits.flatten(start_dim=0, end_dim=1)\n","targets_flat = targets.flatten()\n","print(f\"Flat logits shape: {logits_flat.shape}\")\n","print(f\"Flat targets shape: {targets_flat.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zl0p7fkMedL6","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":8,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"644da954-5879-440c-9cd0-54eb46e256a7"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["Flat logits shape: torch.Size([6, 30000])\n","Flat targets shape: torch.Size([6])\n"]}]},{"cell_type":"code","source":["targets, targets_flat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYClcI-ielK0","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":7,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"b7b0493c-2540-40d2-9d56-10433c2f42f2"},"execution_count":105,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 3626,  6100,   345],\n","         [ 1107,   588, 11311]]),\n"," tensor([ 3626,  6100,   345,  1107,   588, 11311]))"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oko6NodRezgx","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":6,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"12ffe6da-0adc-4e9b-d0fc-ad584eee2d22"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.2699)\n"]}]},{"cell_type":"markdown","source":["Here `PyTorch` will handle the conversion of logits to softmax and then calculate the negative log likelihood for use so we just need to send the probabilities of predicted tokens i.e [6, vocab_size] and the actual target ids [6]."],"metadata":{"id":"dZWF_vM2e-U8"}},{"cell_type":"markdown","source":["### Perplexity\n","It is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling.\n","\n","It can provide a more interpretable way to understand the uncertainity of the model in predicting the next token in a sequence."],"metadata":{"id":"93THwVa9gXAf"}},{"cell_type":"code","source":["perplexity = torch.exp(loss)\n","perplexity"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vlllPXKfSsa","executionInfo":{"status":"ok","timestamp":1734934935741,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"5c9357f4-3c24-4b5a-bb02-9d29b097d40a"},"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(28850.6270)"]},"metadata":{},"execution_count":107}]},{"cell_type":"markdown","source":["## Calculation of validation and Traning error"],"metadata":{"id":"9aIcGL3kjS8v"}},{"cell_type":"code","source":["with open(\"./roman_02.txt\", \"r\", encoding=\"utf-8\") as f:\n","  text = f.read()"],"metadata":{"id":"2jCFGMEijVv5","executionInfo":{"status":"ok","timestamp":1734934936678,"user_tz":-345,"elapsed":941,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["total_characters = len(text)\n","total_tokens = len(tokenizer.encode(text))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaG-hOiBkU3V","executionInfo":{"status":"ok","timestamp":1734934965973,"user_tz":-345,"elapsed":29297,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"14290402-3435-4271-c289-ae6d4cbcd094"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Characters: 37598550\n","Tokens: 6271371\n"]}]},{"cell_type":"code","source":["train_ratio = 0.90\n","split_idx = int(total_characters * train_ratio)\n","train_data = text[:split_idx]\n","val_data = text[split_idx:]"],"metadata":{"id":"iS-b7671k4rx","executionInfo":{"status":"ok","timestamp":1734934965973,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class DatasetV1(Dataset):\n","    def __init__(self, text, tokenizer, max_length, stride):\n","        \"\"\"create a dataset from the txt\"\"\"\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        token_ids = tokenizer.encode(text).ids\n","\n","        for i in range(0, len(token_ids) - max_length, stride): # stride is for overlapping\n","            input_chunk = token_ids[i:i+max_length]\n","            target_chunk = token_ids[i+1:i+max_length+1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","            return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","            return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"Wd6zyLoalj4N","executionInfo":{"status":"ok","timestamp":1734934965973,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["def create_dataloaderV1(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n","    # initializing tokenizer\n","\n","    dataset = DatasetV1(text, tokenizer, max_length, stride)\n","\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size, shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader"],"metadata":{"id":"CVC_dApPlmxQ","executionInfo":{"status":"ok","timestamp":1734934965973,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","train_loader = create_dataloaderV1(\n","    train_data,\n","    batch_size=32,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloaderV1(\n","    val_data,\n","    batch_size=32,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"4B-uhRTdlo7t","executionInfo":{"status":"ok","timestamp":1734934998832,"user_tz":-345,"elapsed":32862,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["# Sanity check\n","if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the training loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"increase the `training_ratio`\")\n","\n","if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the validation loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"decrease the `training_ratio`\")"],"metadata":{"id":"Qi74SwiJlz8-","executionInfo":{"status":"ok","timestamp":1734934998833,"user_tz":-345,"elapsed":6,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["print(\"Train loader:\")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"\\nValidation loader:\")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MfDw1kIOl7Ck","executionInfo":{"status":"ok","timestamp":1734934998833,"user_tz":-345,"elapsed":5,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"83f42bc2-5440-4d2b-f910-cf9c37b98eea"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","\n","Validation loader:\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([32, 512]) torch.Size([32, 512])\n","torch.Size([19, 512]) torch.Size([19, 512])\n"]}]},{"cell_type":"markdown","source":["## Calculate the batch generalization error"],"metadata":{"id":"VsiMKTD1mbeX"}},{"cell_type":"code","source":["def calc_loss_batch(input_batch, target_batch, model, device):\n","  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","  logits = model(input_batch)\n","  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","  return loss"],"metadata":{"id":"e1Q9vPGema2x","executionInfo":{"status":"ok","timestamp":1734934998833,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["def calc_loss_loader(data_loader, model, device, num_batches=None):\n","  total_loss = 0.\n","  if len(data_loader) == 0:\n","    return float(\"nan\")\n","  elif num_batches is None:\n","    num_batches = len(data_loader)\n","  else:\n","    num_batches = min(num_batches, len(data_loader))\n","  for i, (input_batch, target_batch) in enumerate(data_loader):\n","    if i < num_batches:\n","      loss = calc_loss_batch(input_batch, target_batch, model, device)\n","      total_loss = total_loss + loss.item()\n","    else:\n","      break\n","  return total_loss / num_batches"],"metadata":{"id":"5JzpQ_A8m06o","executionInfo":{"status":"ok","timestamp":1734934998833,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","with torch.no_grad():\n","  # train_loss = calc_loss_loader(train_loader, model, device)\n","  val_loss = calc_loss_loader(val_loader, model, device)\n","\n","  # print(f\"Train loss: {train_loss:.3f}\")\n","  print(f\"Validation loss: {val_loss:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCZW8vqLnwgv","executionInfo":{"status":"ok","timestamp":1734935134850,"user_tz":-345,"elapsed":17820,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"e6f0231a-6d7c-422d-e245-358651e991a1"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss: 10.478\n"]}]},{"cell_type":"markdown","source":["## Traning Model"],"metadata":{"id":"AMfjUk5jp_4R"}},{"cell_type":"code","source":["def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","  model.eval()\n","  with torch.no_grad():\n","    # train_loss = calc_loss_loader(train_loader, model, device, eval_iter)\n","    val_loss = calc_loss_loader(val_loader, model, device, eval_iter)\n","  model.train()\n","  return val_loss"],"metadata":{"id":"hmvk3CRVrT8D","executionInfo":{"status":"ok","timestamp":1734935134851,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["def generate_and_print_sample(model, tokenizer, device, start_context):\n","  model.eval()\n","  context_size = model.pos_emb.weight.shape[0]\n","  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","  with torch.no_grad():\n","    token_ids = generate_text_sample(model, encoded, max_new_tokens=50, context_size=context_size)\n","  decoded_text = token_ids_to_text(token_ids, tokenizer)\n","  print(decoded_text.replace(\"\\n\", \" \"))\n","  model.train()"],"metadata":{"id":"n98CBAROrrXD","executionInfo":{"status":"ok","timestamp":1734935134851,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n","  train_losses, val_losses, track_tokens_seen = [], [], []\n","  tokens_seen, global_step = 0, -1\n","\n","  for epoch in range(num_epochs):\n","    model.train()\n","\n","    for input_batch, target_batch in train_loader:\n","      optimizer.zero_grad()\n","\n","      loss = calc_loss_batch(input_batch, target_batch, model, device)\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","      tokens_seen += input_batch.numel()\n","\n","      global_step += 1\n","\n","      if global_step % eval_freq == 0:\n","        val_loss = evaluate_model(\n","            model,\n","            train_loader,\n","            val_loader,\n","            device,\n","            eval_iter\n","        )\n","        loss_ = loss.item()\n","        train_losses.append(loss_)\n","        val_losses.append(val_loss)\n","        track_tokens_seen.append(tokens_seen)\n","        print(f\"Epoch: {epoch} (Step: {global_step:06d}): Train loss: {loss_:.3f}, Val loss: {val_loss:.3f}\")\n","\n","    generate_and_print_sample(\n","        model, tokenizer, device, start_context\n","    )\n","\n","  return train_losses, val_losses, track_tokens_seen\n"],"metadata":{"id":"C6kavt4BqCGv","executionInfo":{"status":"ok","timestamp":1734935184929,"user_tz":-345,"elapsed":564,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":125,"outputs":[]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.1)\n","\n","num_epochs = 15\n","\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model,\n","    train_loader,\n","    val_loader,\n","    optimizer,\n","    device,\n","    num_epochs=num_epochs,\n","    eval_freq=5,\n","    eval_iter=5,\n","    start_context=\"hami sabai\",\n","    tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oU79axZsRLV","executionInfo":{"status":"ok","timestamp":1734943573670,"user_tz":-345,"elapsed":8388113,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"ecf47595-2292-4cee-cf87-34209ddf654c"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 (Step: 000000): Train loss: 10.471, Val loss: 10.388\n","Epoch: 0 (Step: 000005): Train loss: 10.046, Val loss: 9.923\n","Epoch: 0 (Step: 000010): Train loss: 9.573, Val loss: 9.523\n","Epoch: 0 (Step: 000015): Train loss: 9.344, Val loss: 9.212\n","Epoch: 0 (Step: 000020): Train loss: 9.027, Val loss: 8.990\n","Epoch: 0 (Step: 000025): Train loss: 8.842, Val loss: 8.784\n","Epoch: 0 (Step: 000030): Train loss: 8.642, Val loss: 8.622\n","Epoch: 0 (Step: 000035): Train loss: 8.511, Val loss: 8.477\n","Epoch: 0 (Step: 000040): Train loss: 8.385, Val loss: 8.361\n","Epoch: 0 (Step: 000045): Train loss: 8.214, Val loss: 8.265\n","Epoch: 0 (Step: 000050): Train loss: 8.186, Val loss: 8.192\n","Epoch: 0 (Step: 000055): Train loss: 8.165, Val loss: 8.130\n","Epoch: 0 (Step: 000060): Train loss: 8.137, Val loss: 8.081\n","Epoch: 0 (Step: 000065): Train loss: 8.017, Val loss: 8.045\n","Epoch: 0 (Step: 000070): Train loss: 7.999, Val loss: 8.019\n","Epoch: 0 (Step: 000075): Train loss: 8.012, Val loss: 7.999\n","Epoch: 0 (Step: 000080): Train loss: 7.969, Val loss: 7.984\n","Epoch: 0 (Step: 000085): Train loss: 7.947, Val loss: 7.969\n","Epoch: 0 (Step: 000090): Train loss: 7.953, Val loss: 7.959\n","Epoch: 0 (Step: 000095): Train loss: 7.924, Val loss: 7.950\n","Epoch: 0 (Step: 000100): Train loss: 7.963, Val loss: 7.942\n","Epoch: 0 (Step: 000105): Train loss: 7.854, Val loss: 7.932\n","Epoch: 0 (Step: 000110): Train loss: 7.893, Val loss: 7.925\n","Epoch: 0 (Step: 000115): Train loss: 7.894, Val loss: 7.916\n","Epoch: 0 (Step: 000120): Train loss: 7.885, Val loss: 7.909\n","Epoch: 0 (Step: 000125): Train loss: 7.928, Val loss: 7.900\n","Epoch: 0 (Step: 000130): Train loss: 7.936, Val loss: 7.890\n","Epoch: 0 (Step: 000135): Train loss: 7.860, Val loss: 7.883\n","Epoch: 0 (Step: 000140): Train loss: 7.903, Val loss: 7.875\n","Epoch: 0 (Step: 000145): Train loss: 7.950, Val loss: 7.869\n","Epoch: 0 (Step: 000150): Train loss: 7.893, Val loss: 7.863\n","Epoch: 0 (Step: 000155): Train loss: 7.885, Val loss: 7.858\n","Epoch: 0 (Step: 000160): Train loss: 7.821, Val loss: 7.849\n","Epoch: 0 (Step: 000165): Train loss: 7.947, Val loss: 7.842\n","Epoch: 0 (Step: 000170): Train loss: 7.841, Val loss: 7.837\n","Epoch: 0 (Step: 000175): Train loss: 7.789, Val loss: 7.832\n","Epoch: 0 (Step: 000180): Train loss: 7.804, Val loss: 7.827\n","Epoch: 0 (Step: 000185): Train loss: 7.792, Val loss: 7.821\n","Epoch: 0 (Step: 000190): Train loss: 7.753, Val loss: 7.817\n","Epoch: 0 (Step: 000195): Train loss: 7.796, Val loss: 7.811\n","Epoch: 0 (Step: 000200): Train loss: 7.772, Val loss: 7.805\n","Epoch: 0 (Step: 000205): Train loss: 7.689, Val loss: 7.802\n","Epoch: 0 (Step: 000210): Train loss: 7.751, Val loss: 7.795\n","Epoch: 0 (Step: 000215): Train loss: 7.772, Val loss: 7.790\n","Epoch: 0 (Step: 000220): Train loss: 7.789, Val loss: 7.785\n","Epoch: 0 (Step: 000225): Train loss: 7.937, Val loss: 7.781\n","Epoch: 0 (Step: 000230): Train loss: 7.746, Val loss: 7.775\n","Epoch: 0 (Step: 000235): Train loss: 7.715, Val loss: 7.771\n","Epoch: 0 (Step: 000240): Train loss: 7.759, Val loss: 7.766\n","Epoch: 0 (Step: 000245): Train loss: 7.789, Val loss: 7.760\n","Epoch: 0 (Step: 000250): Train loss: 7.737, Val loss: 7.756\n","Epoch: 0 (Step: 000255): Train loss: 7.685, Val loss: 7.751\n","Epoch: 0 (Step: 000260): Train loss: 7.886, Val loss: 7.746\n","Epoch: 0 (Step: 000265): Train loss: 7.759, Val loss: 7.741\n","Epoch: 0 (Step: 000270): Train loss: 7.797, Val loss: 7.737\n","Epoch: 0 (Step: 000275): Train loss: 7.591, Val loss: 7.735\n","Epoch: 0 (Step: 000280): Train loss: 7.678, Val loss: 7.731\n","Epoch: 0 (Step: 000285): Train loss: 7.699, Val loss: 7.727\n","Epoch: 0 (Step: 000290): Train loss: 7.689, Val loss: 7.723\n","Epoch: 0 (Step: 000295): Train loss: 7.726, Val loss: 7.718\n","Epoch: 0 (Step: 000300): Train loss: 7.702, Val loss: 7.713\n","Epoch: 0 (Step: 000305): Train loss: 7.706, Val loss: 7.709\n","Epoch: 0 (Step: 000310): Train loss: 7.686, Val loss: 7.705\n","Epoch: 0 (Step: 000315): Train loss: 7.606, Val loss: 7.702\n","Epoch: 0 (Step: 000320): Train loss: 7.634, Val loss: 7.698\n","Epoch: 0 (Step: 000325): Train loss: 7.742, Val loss: 7.694\n","Epoch: 0 (Step: 000330): Train loss: 7.737, Val loss: 7.690\n","Epoch: 0 (Step: 000335): Train loss: 7.688, Val loss: 7.686\n","Epoch: 0 (Step: 000340): Train loss: 7.783, Val loss: 7.681\n","hami sabai , â€˜ , â€˜ , , â€˜ , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n","Epoch: 1 (Step: 000345): Train loss: 7.717, Val loss: 7.679\n","Epoch: 1 (Step: 000350): Train loss: 7.589, Val loss: 7.677\n","Epoch: 1 (Step: 000355): Train loss: 7.608, Val loss: 7.673\n","Epoch: 1 (Step: 000360): Train loss: 7.584, Val loss: 7.670\n","Epoch: 1 (Step: 000365): Train loss: 7.631, Val loss: 7.666\n","Epoch: 1 (Step: 000370): Train loss: 7.636, Val loss: 7.662\n","Epoch: 1 (Step: 000375): Train loss: 7.683, Val loss: 7.658\n","Epoch: 1 (Step: 000380): Train loss: 7.608, Val loss: 7.654\n","Epoch: 1 (Step: 000385): Train loss: 7.596, Val loss: 7.650\n","Epoch: 1 (Step: 000390): Train loss: 7.578, Val loss: 7.647\n","Epoch: 1 (Step: 000395): Train loss: 7.553, Val loss: 7.641\n","Epoch: 1 (Step: 000400): Train loss: 7.573, Val loss: 7.635\n","Epoch: 1 (Step: 000405): Train loss: 7.674, Val loss: 7.631\n","Epoch: 1 (Step: 000410): Train loss: 7.597, Val loss: 7.631\n","Epoch: 1 (Step: 000415): Train loss: 7.503, Val loss: 7.624\n","Epoch: 1 (Step: 000420): Train loss: 7.665, Val loss: 7.621\n","Epoch: 1 (Step: 000425): Train loss: 7.518, Val loss: 7.614\n","Epoch: 1 (Step: 000430): Train loss: 7.666, Val loss: 7.612\n","Epoch: 1 (Step: 000435): Train loss: 7.575, Val loss: 7.608\n","Epoch: 1 (Step: 000440): Train loss: 7.579, Val loss: 7.603\n","Epoch: 1 (Step: 000445): Train loss: 7.646, Val loss: 7.602\n","Epoch: 1 (Step: 000450): Train loss: 7.591, Val loss: 7.595\n","Epoch: 1 (Step: 000455): Train loss: 7.530, Val loss: 7.591\n","Epoch: 1 (Step: 000460): Train loss: 7.493, Val loss: 7.589\n","Epoch: 1 (Step: 000465): Train loss: 7.532, Val loss: 7.583\n","Epoch: 1 (Step: 000470): Train loss: 7.538, Val loss: 7.579\n","Epoch: 1 (Step: 000475): Train loss: 7.523, Val loss: 7.573\n","Epoch: 1 (Step: 000480): Train loss: 7.597, Val loss: 7.570\n","Epoch: 1 (Step: 000485): Train loss: 7.558, Val loss: 7.564\n","Epoch: 1 (Step: 000490): Train loss: 7.504, Val loss: 7.559\n","Epoch: 1 (Step: 000495): Train loss: 7.572, Val loss: 7.554\n","Epoch: 1 (Step: 000500): Train loss: 7.611, Val loss: 7.552\n","Epoch: 1 (Step: 000505): Train loss: 7.524, Val loss: 7.545\n","Epoch: 1 (Step: 000510): Train loss: 7.447, Val loss: 7.541\n","Epoch: 1 (Step: 000515): Train loss: 7.520, Val loss: 7.537\n","Epoch: 1 (Step: 000520): Train loss: 7.542, Val loss: 7.532\n","Epoch: 1 (Step: 000525): Train loss: 7.479, Val loss: 7.528\n","Epoch: 1 (Step: 000530): Train loss: 7.435, Val loss: 7.524\n","Epoch: 1 (Step: 000535): Train loss: 7.461, Val loss: 7.519\n","Epoch: 1 (Step: 000540): Train loss: 7.567, Val loss: 7.514\n","Epoch: 1 (Step: 000545): Train loss: 7.450, Val loss: 7.510\n","Epoch: 1 (Step: 000550): Train loss: 7.541, Val loss: 7.503\n","Epoch: 1 (Step: 000555): Train loss: 7.550, Val loss: 7.500\n","Epoch: 1 (Step: 000560): Train loss: 7.483, Val loss: 7.493\n","Epoch: 1 (Step: 000565): Train loss: 7.475, Val loss: 7.495\n","Epoch: 1 (Step: 000570): Train loss: 7.446, Val loss: 7.486\n","Epoch: 1 (Step: 000575): Train loss: 7.457, Val loss: 7.485\n","Epoch: 1 (Step: 000580): Train loss: 7.461, Val loss: 7.480\n","Epoch: 1 (Step: 000585): Train loss: 7.505, Val loss: 7.474\n","Epoch: 1 (Step: 000590): Train loss: 7.527, Val loss: 7.468\n","Epoch: 1 (Step: 000595): Train loss: 7.494, Val loss: 7.463\n","Epoch: 1 (Step: 000600): Train loss: 7.424, Val loss: 7.459\n","Epoch: 1 (Step: 000605): Train loss: 7.474, Val loss: 7.453\n","Epoch: 1 (Step: 000610): Train loss: 7.436, Val loss: 7.449\n","Epoch: 1 (Step: 000615): Train loss: 7.347, Val loss: 7.448\n","Epoch: 1 (Step: 000620): Train loss: 7.409, Val loss: 7.444\n","Epoch: 1 (Step: 000625): Train loss: 7.455, Val loss: 7.443\n","Epoch: 1 (Step: 000630): Train loss: 7.455, Val loss: 7.435\n","Epoch: 1 (Step: 000635): Train loss: 7.496, Val loss: 7.430\n","Epoch: 1 (Step: 000640): Train loss: 7.357, Val loss: 7.426\n","Epoch: 1 (Step: 000645): Train loss: 7.352, Val loss: 7.422\n","Epoch: 1 (Step: 000650): Train loss: 7.449, Val loss: 7.415\n","Epoch: 1 (Step: 000655): Train loss: 7.472, Val loss: 7.412\n","Epoch: 1 (Step: 000660): Train loss: 7.315, Val loss: 7.407\n","Epoch: 1 (Step: 000665): Train loss: 7.424, Val loss: 7.406\n","Epoch: 1 (Step: 000670): Train loss: 7.356, Val loss: 7.401\n","Epoch: 1 (Step: 000675): Train loss: 7.473, Val loss: 7.401\n","Epoch: 1 (Step: 000680): Train loss: 7.378, Val loss: 7.395\n","Epoch: 1 (Step: 000685): Train loss: 7.361, Val loss: 7.389\n","hami sabai pani ho . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n\n","Epoch: 2 (Step: 000690): Train loss: 7.372, Val loss: 7.385\n","Epoch: 2 (Step: 000695): Train loss: 7.311, Val loss: 7.380\n","Epoch: 2 (Step: 000700): Train loss: 7.437, Val loss: 7.378\n","Epoch: 2 (Step: 000705): Train loss: 7.391, Val loss: 7.372\n","Epoch: 2 (Step: 000710): Train loss: 7.438, Val loss: 7.368\n","Epoch: 2 (Step: 000715): Train loss: 7.301, Val loss: 7.365\n","Epoch: 2 (Step: 000720): Train loss: 7.295, Val loss: 7.362\n","Epoch: 2 (Step: 000725): Train loss: 7.234, Val loss: 7.358\n","Epoch: 2 (Step: 000730): Train loss: 7.357, Val loss: 7.355\n","Epoch: 2 (Step: 000735): Train loss: 7.264, Val loss: 7.352\n","Epoch: 2 (Step: 000740): Train loss: 7.352, Val loss: 7.342\n","Epoch: 2 (Step: 000745): Train loss: 7.277, Val loss: 7.341\n","Epoch: 2 (Step: 000750): Train loss: 7.328, Val loss: 7.341\n","Epoch: 2 (Step: 000755): Train loss: 7.359, Val loss: 7.335\n","Epoch: 2 (Step: 000760): Train loss: 7.211, Val loss: 7.333\n","Epoch: 2 (Step: 000765): Train loss: 7.328, Val loss: 7.332\n","Epoch: 2 (Step: 000770): Train loss: 7.422, Val loss: 7.328\n","Epoch: 2 (Step: 000775): Train loss: 7.189, Val loss: 7.324\n","Epoch: 2 (Step: 000780): Train loss: 7.314, Val loss: 7.316\n","Epoch: 2 (Step: 000785): Train loss: 7.338, Val loss: 7.318\n","Epoch: 2 (Step: 000790): Train loss: 7.192, Val loss: 7.311\n","Epoch: 2 (Step: 000795): Train loss: 7.246, Val loss: 7.310\n","Epoch: 2 (Step: 000800): Train loss: 7.278, Val loss: 7.302\n","Epoch: 2 (Step: 000805): Train loss: 7.277, Val loss: 7.300\n","Epoch: 2 (Step: 000810): Train loss: 7.246, Val loss: 7.295\n","Epoch: 2 (Step: 000815): Train loss: 7.277, Val loss: 7.299\n","Epoch: 2 (Step: 000820): Train loss: 7.121, Val loss: 7.284\n","Epoch: 2 (Step: 000825): Train loss: 7.260, Val loss: 7.291\n","Epoch: 2 (Step: 000830): Train loss: 7.387, Val loss: 7.279\n","Epoch: 2 (Step: 000835): Train loss: 7.284, Val loss: 7.280\n","Epoch: 2 (Step: 000840): Train loss: 7.167, Val loss: 7.270\n","Epoch: 2 (Step: 000845): Train loss: 7.253, Val loss: 7.269\n","Epoch: 2 (Step: 000850): Train loss: 7.140, Val loss: 7.268\n","Epoch: 2 (Step: 000855): Train loss: 7.200, Val loss: 7.262\n","Epoch: 2 (Step: 000860): Train loss: 7.196, Val loss: 7.265\n","Epoch: 2 (Step: 000865): Train loss: 7.306, Val loss: 7.263\n","Epoch: 2 (Step: 000870): Train loss: 7.220, Val loss: 7.252\n","Epoch: 2 (Step: 000875): Train loss: 7.183, Val loss: 7.253\n","Epoch: 2 (Step: 000880): Train loss: 7.196, Val loss: 7.253\n","Epoch: 2 (Step: 000885): Train loss: 7.245, Val loss: 7.244\n","Epoch: 2 (Step: 000890): Train loss: 7.127, Val loss: 7.241\n","Epoch: 2 (Step: 000895): Train loss: 7.332, Val loss: 7.232\n","Epoch: 2 (Step: 000900): Train loss: 7.257, Val loss: 7.238\n","Epoch: 2 (Step: 000905): Train loss: 7.199, Val loss: 7.231\n","Epoch: 2 (Step: 000910): Train loss: 7.233, Val loss: 7.226\n","Epoch: 2 (Step: 000915): Train loss: 7.198, Val loss: 7.224\n","Epoch: 2 (Step: 000920): Train loss: 7.229, Val loss: 7.220\n","Epoch: 2 (Step: 000925): Train loss: 7.176, Val loss: 7.220\n","Epoch: 2 (Step: 000930): Train loss: 7.274, Val loss: 7.211\n","Epoch: 2 (Step: 000935): Train loss: 7.255, Val loss: 7.212\n","Epoch: 2 (Step: 000940): Train loss: 7.203, Val loss: 7.210\n","Epoch: 2 (Step: 000945): Train loss: 7.195, Val loss: 7.201\n","Epoch: 2 (Step: 000950): Train loss: 7.214, Val loss: 7.207\n","Epoch: 2 (Step: 000955): Train loss: 7.165, Val loss: 7.195\n","Epoch: 2 (Step: 000960): Train loss: 7.094, Val loss: 7.198\n","Epoch: 2 (Step: 000965): Train loss: 7.155, Val loss: 7.191\n","Epoch: 2 (Step: 000970): Train loss: 7.193, Val loss: 7.194\n","Epoch: 2 (Step: 000975): Train loss: 7.095, Val loss: 7.187\n","Epoch: 2 (Step: 000980): Train loss: 7.030, Val loss: 7.186\n","Epoch: 2 (Step: 000985): Train loss: 7.184, Val loss: 7.182\n","Epoch: 2 (Step: 000990): Train loss: 7.141, Val loss: 7.179\n","Epoch: 2 (Step: 000995): Train loss: 7.218, Val loss: 7.176\n","Epoch: 2 (Step: 001000): Train loss: 7.135, Val loss: 7.173\n","Epoch: 2 (Step: 001005): Train loss: 7.125, Val loss: 7.171\n","Epoch: 2 (Step: 001010): Train loss: 7.145, Val loss: 7.167\n","Epoch: 2 (Step: 001015): Train loss: 7.189, Val loss: 7.163\n","Epoch: 2 (Step: 001020): Train loss: 7.238, Val loss: 7.162\n","Epoch: 2 (Step: 001025): Train loss: 7.074, Val loss: 7.159\n","Epoch: 2 (Step: 001030): Train loss: 7.125, Val loss: 7.151\n","hami sabai bhanda pani ho . yo . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n .\n","Epoch: 3 (Step: 001035): Train loss: 6.986, Val loss: 7.154\n","Epoch: 3 (Step: 001040): Train loss: 7.108, Val loss: 7.145\n","Epoch: 3 (Step: 001045): Train loss: 7.039, Val loss: 7.149\n","Epoch: 3 (Step: 001050): Train loss: 7.142, Val loss: 7.145\n","Epoch: 3 (Step: 001055): Train loss: 7.092, Val loss: 7.144\n","Epoch: 3 (Step: 001060): Train loss: 7.208, Val loss: 7.136\n","Epoch: 3 (Step: 001065): Train loss: 7.158, Val loss: 7.137\n","Epoch: 3 (Step: 001070): Train loss: 7.133, Val loss: 7.132\n","Epoch: 3 (Step: 001075): Train loss: 7.173, Val loss: 7.127\n","Epoch: 3 (Step: 001080): Train loss: 7.186, Val loss: 7.126\n","Epoch: 3 (Step: 001085): Train loss: 7.086, Val loss: 7.121\n","Epoch: 3 (Step: 001090): Train loss: 7.000, Val loss: 7.120\n","Epoch: 3 (Step: 001095): Train loss: 7.182, Val loss: 7.115\n","Epoch: 3 (Step: 001100): Train loss: 7.119, Val loss: 7.117\n","Epoch: 3 (Step: 001105): Train loss: 7.021, Val loss: 7.118\n","Epoch: 3 (Step: 001110): Train loss: 6.906, Val loss: 7.113\n","Epoch: 3 (Step: 001115): Train loss: 7.040, Val loss: 7.106\n","Epoch: 3 (Step: 001120): Train loss: 6.942, Val loss: 7.105\n","Epoch: 3 (Step: 001125): Train loss: 7.083, Val loss: 7.105\n","Epoch: 3 (Step: 001130): Train loss: 7.061, Val loss: 7.099\n","Epoch: 3 (Step: 001135): Train loss: 7.163, Val loss: 7.103\n","Epoch: 3 (Step: 001140): Train loss: 6.961, Val loss: 7.095\n","Epoch: 3 (Step: 001145): Train loss: 6.954, Val loss: 7.089\n","Epoch: 3 (Step: 001150): Train loss: 7.041, Val loss: 7.091\n","Epoch: 3 (Step: 001155): Train loss: 7.017, Val loss: 7.085\n","Epoch: 3 (Step: 001160): Train loss: 7.031, Val loss: 7.082\n","Epoch: 3 (Step: 001165): Train loss: 6.968, Val loss: 7.076\n","Epoch: 3 (Step: 001170): Train loss: 7.099, Val loss: 7.073\n","Epoch: 3 (Step: 001175): Train loss: 7.181, Val loss: 7.086\n","Epoch: 3 (Step: 001180): Train loss: 7.059, Val loss: 7.070\n","Epoch: 3 (Step: 001185): Train loss: 6.936, Val loss: 7.072\n","Epoch: 3 (Step: 001190): Train loss: 7.039, Val loss: 7.073\n","Epoch: 3 (Step: 001195): Train loss: 6.989, Val loss: 7.060\n","Epoch: 3 (Step: 001200): Train loss: 7.015, Val loss: 7.061\n","Epoch: 3 (Step: 001205): Train loss: 6.892, Val loss: 7.058\n","Epoch: 3 (Step: 001210): Train loss: 6.939, Val loss: 7.051\n","Epoch: 3 (Step: 001215): Train loss: 6.981, Val loss: 7.052\n","Epoch: 3 (Step: 001220): Train loss: 6.995, Val loss: 7.047\n","Epoch: 3 (Step: 001225): Train loss: 6.992, Val loss: 7.051\n","Epoch: 3 (Step: 001230): Train loss: 6.917, Val loss: 7.046\n","Epoch: 3 (Step: 001235): Train loss: 7.126, Val loss: 7.038\n","Epoch: 3 (Step: 001240): Train loss: 6.963, Val loss: 7.044\n","Epoch: 3 (Step: 001245): Train loss: 7.053, Val loss: 7.033\n","Epoch: 3 (Step: 001250): Train loss: 6.900, Val loss: 7.036\n","Epoch: 3 (Step: 001255): Train loss: 7.041, Val loss: 7.030\n","Epoch: 3 (Step: 001260): Train loss: 6.916, Val loss: 7.031\n","Epoch: 3 (Step: 001265): Train loss: 6.913, Val loss: 7.031\n","Epoch: 3 (Step: 001270): Train loss: 7.013, Val loss: 7.034\n","Epoch: 3 (Step: 001275): Train loss: 6.937, Val loss: 7.036\n","Epoch: 3 (Step: 001280): Train loss: 6.996, Val loss: 7.021\n","Epoch: 3 (Step: 001285): Train loss: 6.974, Val loss: 7.014\n","Epoch: 3 (Step: 001290): Train loss: 6.996, Val loss: 7.011\n","Epoch: 3 (Step: 001295): Train loss: 7.047, Val loss: 7.009\n","Epoch: 3 (Step: 001300): Train loss: 6.911, Val loss: 7.007\n","Epoch: 3 (Step: 001305): Train loss: 7.002, Val loss: 7.002\n","Epoch: 3 (Step: 001310): Train loss: 7.039, Val loss: 7.001\n","Epoch: 3 (Step: 001315): Train loss: 6.881, Val loss: 7.005\n","Epoch: 3 (Step: 001320): Train loss: 6.956, Val loss: 7.011\n","Epoch: 3 (Step: 001325): Train loss: 6.986, Val loss: 6.998\n","Epoch: 3 (Step: 001330): Train loss: 6.899, Val loss: 6.995\n","Epoch: 3 (Step: 001335): Train loss: 6.932, Val loss: 6.993\n","Epoch: 3 (Step: 001340): Train loss: 6.897, Val loss: 6.989\n","Epoch: 3 (Step: 001345): Train loss: 7.004, Val loss: 6.992\n","Epoch: 3 (Step: 001350): Train loss: 6.920, Val loss: 6.986\n","Epoch: 3 (Step: 001355): Train loss: 6.941, Val loss: 6.983\n","Epoch: 3 (Step: 001360): Train loss: 6.844, Val loss: 6.982\n","Epoch: 3 (Step: 001365): Train loss: 6.882, Val loss: 6.977\n","Epoch: 3 (Step: 001370): Train loss: 6.856, Val loss: 6.978\n","Epoch: 3 (Step: 001375): Train loss: 7.021, Val loss: 6.971\n","hami sabai bhanda pani ho . yo . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n .\n","Epoch: 4 (Step: 001380): Train loss: 6.862, Val loss: 6.976\n","Epoch: 4 (Step: 001385): Train loss: 6.979, Val loss: 6.969\n","Epoch: 4 (Step: 001390): Train loss: 7.027, Val loss: 6.964\n","Epoch: 4 (Step: 001395): Train loss: 6.919, Val loss: 6.969\n","Epoch: 4 (Step: 001400): Train loss: 6.823, Val loss: 6.964\n","Epoch: 4 (Step: 001405): Train loss: 6.806, Val loss: 6.963\n","Epoch: 4 (Step: 001410): Train loss: 6.858, Val loss: 6.964\n","Epoch: 4 (Step: 001415): Train loss: 6.941, Val loss: 6.955\n","Epoch: 4 (Step: 001420): Train loss: 6.907, Val loss: 6.960\n","Epoch: 4 (Step: 001425): Train loss: 6.885, Val loss: 6.960\n","Epoch: 4 (Step: 001430): Train loss: 6.670, Val loss: 6.949\n","Epoch: 4 (Step: 001435): Train loss: 6.960, Val loss: 6.945\n","Epoch: 4 (Step: 001440): Train loss: 6.918, Val loss: 6.939\n","Epoch: 4 (Step: 001445): Train loss: 6.790, Val loss: 6.944\n","Epoch: 4 (Step: 001450): Train loss: 6.824, Val loss: 6.938\n","Epoch: 4 (Step: 001455): Train loss: 6.872, Val loss: 6.935\n","Epoch: 4 (Step: 001460): Train loss: 6.852, Val loss: 6.938\n","Epoch: 4 (Step: 001465): Train loss: 6.873, Val loss: 6.931\n","Epoch: 4 (Step: 001470): Train loss: 6.815, Val loss: 6.929\n","Epoch: 4 (Step: 001475): Train loss: 6.793, Val loss: 6.936\n","Epoch: 4 (Step: 001480): Train loss: 6.707, Val loss: 6.932\n","Epoch: 4 (Step: 001485): Train loss: 6.855, Val loss: 6.930\n","Epoch: 4 (Step: 001490): Train loss: 6.857, Val loss: 6.931\n","Epoch: 4 (Step: 001495): Train loss: 6.766, Val loss: 6.926\n","Epoch: 4 (Step: 001500): Train loss: 6.877, Val loss: 6.915\n","Epoch: 4 (Step: 001505): Train loss: 6.782, Val loss: 6.919\n","Epoch: 4 (Step: 001510): Train loss: 6.895, Val loss: 6.910\n","Epoch: 4 (Step: 001515): Train loss: 6.775, Val loss: 6.914\n","Epoch: 4 (Step: 001520): Train loss: 6.888, Val loss: 6.911\n","Epoch: 4 (Step: 001525): Train loss: 6.803, Val loss: 6.906\n","Epoch: 4 (Step: 001530): Train loss: 6.745, Val loss: 6.908\n","Epoch: 4 (Step: 001535): Train loss: 6.812, Val loss: 6.903\n","Epoch: 4 (Step: 001540): Train loss: 6.894, Val loss: 6.902\n","Epoch: 4 (Step: 001545): Train loss: 6.859, Val loss: 6.895\n","Epoch: 4 (Step: 001550): Train loss: 6.824, Val loss: 6.893\n","Epoch: 4 (Step: 001555): Train loss: 6.861, Val loss: 6.890\n","Epoch: 4 (Step: 001560): Train loss: 6.791, Val loss: 6.897\n","Epoch: 4 (Step: 001565): Train loss: 6.796, Val loss: 6.893\n","Epoch: 4 (Step: 001570): Train loss: 6.809, Val loss: 6.893\n","Epoch: 4 (Step: 001575): Train loss: 6.716, Val loss: 6.889\n","Epoch: 4 (Step: 001580): Train loss: 6.849, Val loss: 6.889\n","Epoch: 4 (Step: 001585): Train loss: 6.847, Val loss: 6.875\n","Epoch: 4 (Step: 001590): Train loss: 6.803, Val loss: 6.878\n","Epoch: 4 (Step: 001595): Train loss: 6.770, Val loss: 6.881\n","Epoch: 4 (Step: 001600): Train loss: 6.707, Val loss: 6.866\n","Epoch: 4 (Step: 001605): Train loss: 6.807, Val loss: 6.869\n","Epoch: 4 (Step: 001610): Train loss: 6.785, Val loss: 6.875\n","Epoch: 4 (Step: 001615): Train loss: 6.692, Val loss: 6.867\n","Epoch: 4 (Step: 001620): Train loss: 6.889, Val loss: 6.870\n","Epoch: 4 (Step: 001625): Train loss: 6.701, Val loss: 6.863\n","Epoch: 4 (Step: 001630): Train loss: 6.789, Val loss: 6.858\n","Epoch: 4 (Step: 001635): Train loss: 6.783, Val loss: 6.857\n","Epoch: 4 (Step: 001640): Train loss: 6.872, Val loss: 6.855\n","Epoch: 4 (Step: 001645): Train loss: 6.767, Val loss: 6.852\n","Epoch: 4 (Step: 001650): Train loss: 6.762, Val loss: 6.852\n","Epoch: 4 (Step: 001655): Train loss: 6.827, Val loss: 6.844\n","Epoch: 4 (Step: 001660): Train loss: 6.727, Val loss: 6.843\n","Epoch: 4 (Step: 001665): Train loss: 6.838, Val loss: 6.848\n","Epoch: 4 (Step: 001670): Train loss: 6.736, Val loss: 6.835\n","Epoch: 4 (Step: 001675): Train loss: 6.763, Val loss: 6.836\n","Epoch: 4 (Step: 001680): Train loss: 6.822, Val loss: 6.837\n","Epoch: 4 (Step: 001685): Train loss: 6.629, Val loss: 6.842\n","Epoch: 4 (Step: 001690): Train loss: 6.720, Val loss: 6.836\n","Epoch: 4 (Step: 001695): Train loss: 6.816, Val loss: 6.833\n","Epoch: 4 (Step: 001700): Train loss: 6.776, Val loss: 6.839\n","Epoch: 4 (Step: 001705): Train loss: 6.627, Val loss: 6.828\n","Epoch: 4 (Step: 001710): Train loss: 6.767, Val loss: 6.823\n","Epoch: 4 (Step: 001715): Train loss: 6.917, Val loss: 6.826\n","hami sabai kura ho . tara , tara , yo , yo , yo , yo , yo , yo , yo , yo , yo , tapaim , yo , yo , tapaim , tapaim , tapaim , ra , tapaim , ra , ra , ra , ra , ra\n","Epoch: 5 (Step: 001720): Train loss: 6.562, Val loss: 6.824\n","Epoch: 5 (Step: 001725): Train loss: 6.664, Val loss: 6.827\n","Epoch: 5 (Step: 001730): Train loss: 6.728, Val loss: 6.826\n","Epoch: 5 (Step: 001735): Train loss: 6.754, Val loss: 6.817\n","Epoch: 5 (Step: 001740): Train loss: 6.687, Val loss: 6.816\n","Epoch: 5 (Step: 001745): Train loss: 6.729, Val loss: 6.808\n","Epoch: 5 (Step: 001750): Train loss: 6.700, Val loss: 6.809\n","Epoch: 5 (Step: 001755): Train loss: 6.650, Val loss: 6.808\n","Epoch: 5 (Step: 001760): Train loss: 6.697, Val loss: 6.805\n","Epoch: 5 (Step: 001765): Train loss: 6.762, Val loss: 6.797\n","Epoch: 5 (Step: 001770): Train loss: 6.722, Val loss: 6.808\n","Epoch: 5 (Step: 001775): Train loss: 6.787, Val loss: 6.794\n","Epoch: 5 (Step: 001780): Train loss: 6.761, Val loss: 6.793\n","Epoch: 5 (Step: 001785): Train loss: 6.737, Val loss: 6.791\n","Epoch: 5 (Step: 001790): Train loss: 6.716, Val loss: 6.794\n","Epoch: 5 (Step: 001795): Train loss: 6.688, Val loss: 6.791\n","Epoch: 5 (Step: 001800): Train loss: 6.782, Val loss: 6.786\n","Epoch: 5 (Step: 001805): Train loss: 6.633, Val loss: 6.786\n","Epoch: 5 (Step: 001810): Train loss: 6.637, Val loss: 6.797\n","Epoch: 5 (Step: 001815): Train loss: 6.678, Val loss: 6.779\n","Epoch: 5 (Step: 001820): Train loss: 6.702, Val loss: 6.783\n","Epoch: 5 (Step: 001825): Train loss: 6.736, Val loss: 6.778\n","Epoch: 5 (Step: 001830): Train loss: 6.702, Val loss: 6.774\n","Epoch: 5 (Step: 001835): Train loss: 6.660, Val loss: 6.790\n","Epoch: 5 (Step: 001840): Train loss: 6.694, Val loss: 6.779\n","Epoch: 5 (Step: 001845): Train loss: 6.690, Val loss: 6.770\n","Epoch: 5 (Step: 001850): Train loss: 6.666, Val loss: 6.772\n","Epoch: 5 (Step: 001855): Train loss: 6.646, Val loss: 6.766\n","Epoch: 5 (Step: 001860): Train loss: 6.631, Val loss: 6.759\n","Epoch: 5 (Step: 001865): Train loss: 6.689, Val loss: 6.769\n","Epoch: 5 (Step: 001870): Train loss: 6.637, Val loss: 6.759\n","Epoch: 5 (Step: 001875): Train loss: 6.656, Val loss: 6.753\n","Epoch: 5 (Step: 001880): Train loss: 6.608, Val loss: 6.759\n","Epoch: 5 (Step: 001885): Train loss: 6.750, Val loss: 6.753\n","Epoch: 5 (Step: 001890): Train loss: 6.715, Val loss: 6.749\n","Epoch: 5 (Step: 001895): Train loss: 6.641, Val loss: 6.762\n","Epoch: 5 (Step: 001900): Train loss: 6.543, Val loss: 6.754\n","Epoch: 5 (Step: 001905): Train loss: 6.705, Val loss: 6.744\n","Epoch: 5 (Step: 001910): Train loss: 6.733, Val loss: 6.741\n","Epoch: 5 (Step: 001915): Train loss: 6.704, Val loss: 6.744\n","Epoch: 5 (Step: 001920): Train loss: 6.568, Val loss: 6.739\n","Epoch: 5 (Step: 001925): Train loss: 6.709, Val loss: 6.741\n","Epoch: 5 (Step: 001930): Train loss: 6.720, Val loss: 6.737\n","Epoch: 5 (Step: 001935): Train loss: 6.738, Val loss: 6.739\n","Epoch: 5 (Step: 001940): Train loss: 6.561, Val loss: 6.726\n","Epoch: 5 (Step: 001945): Train loss: 6.583, Val loss: 6.727\n","Epoch: 5 (Step: 001950): Train loss: 6.605, Val loss: 6.726\n","Epoch: 5 (Step: 001955): Train loss: 6.672, Val loss: 6.728\n","Epoch: 5 (Step: 001960): Train loss: 6.642, Val loss: 6.726\n","Epoch: 5 (Step: 001965): Train loss: 6.648, Val loss: 6.716\n","Epoch: 5 (Step: 001970): Train loss: 6.653, Val loss: 6.725\n","Epoch: 5 (Step: 001975): Train loss: 6.642, Val loss: 6.717\n","Epoch: 5 (Step: 001980): Train loss: 6.512, Val loss: 6.721\n","Epoch: 5 (Step: 001985): Train loss: 6.629, Val loss: 6.713\n","Epoch: 5 (Step: 001990): Train loss: 6.583, Val loss: 6.710\n","Epoch: 5 (Step: 001995): Train loss: 6.729, Val loss: 6.705\n","Epoch: 5 (Step: 002000): Train loss: 6.641, Val loss: 6.705\n","Epoch: 5 (Step: 002005): Train loss: 6.530, Val loss: 6.704\n","Epoch: 5 (Step: 002010): Train loss: 6.610, Val loss: 6.700\n","Epoch: 5 (Step: 002015): Train loss: 6.622, Val loss: 6.700\n","Epoch: 5 (Step: 002020): Train loss: 6.531, Val loss: 6.694\n","Epoch: 5 (Step: 002025): Train loss: 6.610, Val loss: 6.696\n","Epoch: 5 (Step: 002030): Train loss: 6.539, Val loss: 6.692\n","Epoch: 5 (Step: 002035): Train loss: 6.528, Val loss: 6.692\n","Epoch: 5 (Step: 002040): Train loss: 6.689, Val loss: 6.688\n","Epoch: 5 (Step: 002045): Train loss: 6.542, Val loss: 6.692\n","Epoch: 5 (Step: 002050): Train loss: 6.537, Val loss: 6.698\n","Epoch: 5 (Step: 002055): Train loss: 6.619, Val loss: 6.688\n","Epoch: 5 (Step: 002060): Train loss: 6.598, Val loss: 6.683\n","hami sabai bhanda badhi cha . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo . yo . yo . yo . yo kura ho . yo . yo kura ho . yo . yo . yo kura ho .\n","Epoch: 6 (Step: 002065): Train loss: 6.595, Val loss: 6.677\n","Epoch: 6 (Step: 002070): Train loss: 6.640, Val loss: 6.674\n","Epoch: 6 (Step: 002075): Train loss: 6.465, Val loss: 6.673\n","Epoch: 6 (Step: 002080): Train loss: 6.515, Val loss: 6.674\n","Epoch: 6 (Step: 002085): Train loss: 6.587, Val loss: 6.673\n","Epoch: 6 (Step: 002090): Train loss: 6.613, Val loss: 6.668\n","Epoch: 6 (Step: 002095): Train loss: 6.551, Val loss: 6.673\n","Epoch: 6 (Step: 002100): Train loss: 6.432, Val loss: 6.671\n","Epoch: 6 (Step: 002105): Train loss: 6.499, Val loss: 6.667\n","Epoch: 6 (Step: 002110): Train loss: 6.500, Val loss: 6.664\n","Epoch: 6 (Step: 002115): Train loss: 6.526, Val loss: 6.662\n","Epoch: 6 (Step: 002120): Train loss: 6.518, Val loss: 6.657\n","Epoch: 6 (Step: 002125): Train loss: 6.556, Val loss: 6.657\n","Epoch: 6 (Step: 002130): Train loss: 6.523, Val loss: 6.656\n","Epoch: 6 (Step: 002135): Train loss: 6.556, Val loss: 6.660\n","Epoch: 6 (Step: 002140): Train loss: 6.613, Val loss: 6.650\n","Epoch: 6 (Step: 002145): Train loss: 6.494, Val loss: 6.651\n","Epoch: 6 (Step: 002150): Train loss: 6.543, Val loss: 6.648\n","Epoch: 6 (Step: 002155): Train loss: 6.504, Val loss: 6.644\n","Epoch: 6 (Step: 002160): Train loss: 6.556, Val loss: 6.654\n","Epoch: 6 (Step: 002165): Train loss: 6.504, Val loss: 6.644\n","Epoch: 6 (Step: 002170): Train loss: 6.529, Val loss: 6.652\n","Epoch: 6 (Step: 002175): Train loss: 6.473, Val loss: 6.638\n","Epoch: 6 (Step: 002180): Train loss: 6.485, Val loss: 6.636\n","Epoch: 6 (Step: 002185): Train loss: 6.439, Val loss: 6.640\n","Epoch: 6 (Step: 002190): Train loss: 6.524, Val loss: 6.642\n","Epoch: 6 (Step: 002195): Train loss: 6.563, Val loss: 6.642\n","Epoch: 6 (Step: 002200): Train loss: 6.518, Val loss: 6.639\n","Epoch: 6 (Step: 002205): Train loss: 6.399, Val loss: 6.629\n","Epoch: 6 (Step: 002210): Train loss: 6.489, Val loss: 6.632\n","Epoch: 6 (Step: 002215): Train loss: 6.459, Val loss: 6.635\n","Epoch: 6 (Step: 002220): Train loss: 6.491, Val loss: 6.630\n","Epoch: 6 (Step: 002225): Train loss: 6.615, Val loss: 6.624\n","Epoch: 6 (Step: 002230): Train loss: 6.567, Val loss: 6.622\n","Epoch: 6 (Step: 002235): Train loss: 6.510, Val loss: 6.621\n","Epoch: 6 (Step: 002240): Train loss: 6.530, Val loss: 6.620\n","Epoch: 6 (Step: 002245): Train loss: 6.557, Val loss: 6.615\n","Epoch: 6 (Step: 002250): Train loss: 6.424, Val loss: 6.617\n","Epoch: 6 (Step: 002255): Train loss: 6.383, Val loss: 6.617\n","Epoch: 6 (Step: 002260): Train loss: 6.531, Val loss: 6.623\n","Epoch: 6 (Step: 002265): Train loss: 6.484, Val loss: 6.615\n","Epoch: 6 (Step: 002270): Train loss: 6.546, Val loss: 6.610\n","Epoch: 6 (Step: 002275): Train loss: 6.368, Val loss: 6.606\n","Epoch: 6 (Step: 002280): Train loss: 6.453, Val loss: 6.606\n","Epoch: 6 (Step: 002285): Train loss: 6.536, Val loss: 6.605\n","Epoch: 6 (Step: 002290): Train loss: 6.547, Val loss: 6.602\n","Epoch: 6 (Step: 002295): Train loss: 6.512, Val loss: 6.602\n","Epoch: 6 (Step: 002300): Train loss: 6.465, Val loss: 6.610\n","Epoch: 6 (Step: 002305): Train loss: 6.562, Val loss: 6.597\n","Epoch: 6 (Step: 002310): Train loss: 6.504, Val loss: 6.589\n","Epoch: 6 (Step: 002315): Train loss: 6.502, Val loss: 6.587\n","Epoch: 6 (Step: 002320): Train loss: 6.454, Val loss: 6.588\n","Epoch: 6 (Step: 002325): Train loss: 6.460, Val loss: 6.591\n","Epoch: 6 (Step: 002330): Train loss: 6.376, Val loss: 6.590\n","Epoch: 6 (Step: 002335): Train loss: 6.384, Val loss: 6.585\n","Epoch: 6 (Step: 002340): Train loss: 6.500, Val loss: 6.587\n","Epoch: 6 (Step: 002345): Train loss: 6.548, Val loss: 6.585\n","Epoch: 6 (Step: 002350): Train loss: 6.446, Val loss: 6.574\n","Epoch: 6 (Step: 002355): Train loss: 6.495, Val loss: 6.577\n","Epoch: 6 (Step: 002360): Train loss: 6.419, Val loss: 6.581\n","Epoch: 6 (Step: 002365): Train loss: 6.411, Val loss: 6.578\n","Epoch: 6 (Step: 002370): Train loss: 6.528, Val loss: 6.574\n","Epoch: 6 (Step: 002375): Train loss: 6.473, Val loss: 6.571\n","Epoch: 6 (Step: 002380): Train loss: 6.396, Val loss: 6.567\n","Epoch: 6 (Step: 002385): Train loss: 6.384, Val loss: 6.565\n","Epoch: 6 (Step: 002390): Train loss: 6.557, Val loss: 6.557\n","Epoch: 6 (Step: 002395): Train loss: 6.328, Val loss: 6.560\n","Epoch: 6 (Step: 002400): Train loss: 6.385, Val loss: 6.564\n","Epoch: 6 (Step: 002405): Train loss: 6.521, Val loss: 6.561\n","hami sabai bhanda badhi cha . yo kura ho . tara , tyo kura ho . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n . n .\n","Epoch: 7 (Step: 002410): Train loss: 6.288, Val loss: 6.565\n","Epoch: 7 (Step: 002415): Train loss: 6.417, Val loss: 6.557\n","Epoch: 7 (Step: 002420): Train loss: 6.355, Val loss: 6.559\n","Epoch: 7 (Step: 002425): Train loss: 6.354, Val loss: 6.558\n","Epoch: 7 (Step: 002430): Train loss: 6.482, Val loss: 6.557\n","Epoch: 7 (Step: 002435): Train loss: 6.492, Val loss: 6.551\n","Epoch: 7 (Step: 002440): Train loss: 6.399, Val loss: 6.557\n","Epoch: 7 (Step: 002445): Train loss: 6.416, Val loss: 6.554\n","Epoch: 7 (Step: 002450): Train loss: 6.450, Val loss: 6.546\n","Epoch: 7 (Step: 002455): Train loss: 6.385, Val loss: 6.545\n","Epoch: 7 (Step: 002460): Train loss: 6.312, Val loss: 6.548\n","Epoch: 7 (Step: 002465): Train loss: 6.369, Val loss: 6.555\n","Epoch: 7 (Step: 002470): Train loss: 6.291, Val loss: 6.549\n","Epoch: 7 (Step: 002475): Train loss: 6.486, Val loss: 6.539\n","Epoch: 7 (Step: 002480): Train loss: 6.388, Val loss: 6.534\n","Epoch: 7 (Step: 002485): Train loss: 6.381, Val loss: 6.540\n","Epoch: 7 (Step: 002490): Train loss: 6.329, Val loss: 6.535\n","Epoch: 7 (Step: 002495): Train loss: 6.324, Val loss: 6.543\n","Epoch: 7 (Step: 002500): Train loss: 6.403, Val loss: 6.539\n","Epoch: 7 (Step: 002505): Train loss: 6.348, Val loss: 6.539\n","Epoch: 7 (Step: 002510): Train loss: 6.463, Val loss: 6.537\n","Epoch: 7 (Step: 002515): Train loss: 6.395, Val loss: 6.534\n","Epoch: 7 (Step: 002520): Train loss: 6.396, Val loss: 6.536\n","Epoch: 7 (Step: 002525): Train loss: 6.341, Val loss: 6.534\n","Epoch: 7 (Step: 002530): Train loss: 6.339, Val loss: 6.526\n","Epoch: 7 (Step: 002535): Train loss: 6.274, Val loss: 6.519\n","Epoch: 7 (Step: 002540): Train loss: 6.460, Val loss: 6.523\n","Epoch: 7 (Step: 002545): Train loss: 6.282, Val loss: 6.519\n","Epoch: 7 (Step: 002550): Train loss: 6.314, Val loss: 6.521\n","Epoch: 7 (Step: 002555): Train loss: 6.415, Val loss: 6.523\n","Epoch: 7 (Step: 002560): Train loss: 6.375, Val loss: 6.521\n","Epoch: 7 (Step: 002565): Train loss: 6.230, Val loss: 6.516\n","Epoch: 7 (Step: 002570): Train loss: 6.265, Val loss: 6.509\n","Epoch: 7 (Step: 002575): Train loss: 6.365, Val loss: 6.512\n","Epoch: 7 (Step: 002580): Train loss: 6.371, Val loss: 6.515\n","Epoch: 7 (Step: 002585): Train loss: 6.409, Val loss: 6.513\n","Epoch: 7 (Step: 002590): Train loss: 6.559, Val loss: 6.507\n","Epoch: 7 (Step: 002595): Train loss: 6.414, Val loss: 6.498\n","Epoch: 7 (Step: 002600): Train loss: 6.395, Val loss: 6.512\n","Epoch: 7 (Step: 002605): Train loss: 6.521, Val loss: 6.502\n","Epoch: 7 (Step: 002610): Train loss: 6.295, Val loss: 6.498\n","Epoch: 7 (Step: 002615): Train loss: 6.386, Val loss: 6.493\n","Epoch: 7 (Step: 002620): Train loss: 6.416, Val loss: 6.502\n","Epoch: 7 (Step: 002625): Train loss: 6.310, Val loss: 6.514\n","Epoch: 7 (Step: 002630): Train loss: 6.335, Val loss: 6.508\n","Epoch: 7 (Step: 002635): Train loss: 6.392, Val loss: 6.495\n","Epoch: 7 (Step: 002640): Train loss: 6.332, Val loss: 6.491\n","Epoch: 7 (Step: 002645): Train loss: 6.414, Val loss: 6.487\n","Epoch: 7 (Step: 002650): Train loss: 6.382, Val loss: 6.491\n","Epoch: 7 (Step: 002655): Train loss: 6.246, Val loss: 6.492\n","Epoch: 7 (Step: 002660): Train loss: 6.339, Val loss: 6.481\n","Epoch: 7 (Step: 002665): Train loss: 6.268, Val loss: 6.482\n","Epoch: 7 (Step: 002670): Train loss: 6.320, Val loss: 6.489\n","Epoch: 7 (Step: 002675): Train loss: 6.314, Val loss: 6.481\n","Epoch: 7 (Step: 002680): Train loss: 6.268, Val loss: 6.480\n","Epoch: 7 (Step: 002685): Train loss: 6.345, Val loss: 6.469\n","Epoch: 7 (Step: 002690): Train loss: 6.315, Val loss: 6.472\n","Epoch: 7 (Step: 002695): Train loss: 6.418, Val loss: 6.472\n","Epoch: 7 (Step: 002700): Train loss: 6.358, Val loss: 6.478\n","Epoch: 7 (Step: 002705): Train loss: 6.285, Val loss: 6.470\n","Epoch: 7 (Step: 002710): Train loss: 6.347, Val loss: 6.462\n","Epoch: 7 (Step: 002715): Train loss: 6.377, Val loss: 6.464\n","Epoch: 7 (Step: 002720): Train loss: 6.323, Val loss: 6.460\n","Epoch: 7 (Step: 002725): Train loss: 6.291, Val loss: 6.467\n","Epoch: 7 (Step: 002730): Train loss: 6.408, Val loss: 6.466\n","Epoch: 7 (Step: 002735): Train loss: 6.310, Val loss: 6.460\n","Epoch: 7 (Step: 002740): Train loss: 6.138, Val loss: 6.463\n","Epoch: 7 (Step: 002745): Train loss: 6.394, Val loss: 6.461\n","Epoch: 7 (Step: 002750): Train loss: 6.355, Val loss: 6.463\n","hami sabai bhanda dherai chan . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho .\n","Epoch: 8 (Step: 002755): Train loss: 6.256, Val loss: 6.455\n","Epoch: 8 (Step: 002760): Train loss: 6.260, Val loss: 6.452\n","Epoch: 8 (Step: 002765): Train loss: 6.327, Val loss: 6.451\n","Epoch: 8 (Step: 002770): Train loss: 6.089, Val loss: 6.451\n","Epoch: 8 (Step: 002775): Train loss: 6.229, Val loss: 6.457\n","Epoch: 8 (Step: 002780): Train loss: 6.156, Val loss: 6.452\n","Epoch: 8 (Step: 002785): Train loss: 6.209, Val loss: 6.445\n","Epoch: 8 (Step: 002790): Train loss: 6.263, Val loss: 6.448\n","Epoch: 8 (Step: 002795): Train loss: 6.378, Val loss: 6.445\n","Epoch: 8 (Step: 002800): Train loss: 6.203, Val loss: 6.451\n","Epoch: 8 (Step: 002805): Train loss: 6.375, Val loss: 6.447\n","Epoch: 8 (Step: 002810): Train loss: 6.261, Val loss: 6.442\n","Epoch: 8 (Step: 002815): Train loss: 6.203, Val loss: 6.439\n","Epoch: 8 (Step: 002820): Train loss: 6.321, Val loss: 6.441\n","Epoch: 8 (Step: 002825): Train loss: 6.236, Val loss: 6.439\n","Epoch: 8 (Step: 002830): Train loss: 6.155, Val loss: 6.443\n","Epoch: 8 (Step: 002835): Train loss: 6.391, Val loss: 6.441\n","Epoch: 8 (Step: 002840): Train loss: 6.245, Val loss: 6.440\n","Epoch: 8 (Step: 002845): Train loss: 6.324, Val loss: 6.440\n","Epoch: 8 (Step: 002850): Train loss: 6.362, Val loss: 6.436\n","Epoch: 8 (Step: 002855): Train loss: 6.321, Val loss: 6.437\n","Epoch: 8 (Step: 002860): Train loss: 6.189, Val loss: 6.433\n","Epoch: 8 (Step: 002865): Train loss: 6.335, Val loss: 6.429\n","Epoch: 8 (Step: 002870): Train loss: 6.307, Val loss: 6.429\n","Epoch: 8 (Step: 002875): Train loss: 6.244, Val loss: 6.424\n","Epoch: 8 (Step: 002880): Train loss: 6.057, Val loss: 6.420\n","Epoch: 8 (Step: 002885): Train loss: 6.237, Val loss: 6.423\n","Epoch: 8 (Step: 002890): Train loss: 6.201, Val loss: 6.429\n","Epoch: 8 (Step: 002895): Train loss: 6.339, Val loss: 6.416\n","Epoch: 8 (Step: 002900): Train loss: 6.153, Val loss: 6.425\n","Epoch: 8 (Step: 002905): Train loss: 6.296, Val loss: 6.419\n","Epoch: 8 (Step: 002910): Train loss: 6.242, Val loss: 6.420\n","Epoch: 8 (Step: 002915): Train loss: 6.289, Val loss: 6.421\n","Epoch: 8 (Step: 002920): Train loss: 6.296, Val loss: 6.412\n","Epoch: 8 (Step: 002925): Train loss: 6.187, Val loss: 6.417\n","Epoch: 8 (Step: 002930): Train loss: 6.366, Val loss: 6.410\n","Epoch: 8 (Step: 002935): Train loss: 6.289, Val loss: 6.408\n","Epoch: 8 (Step: 002940): Train loss: 6.147, Val loss: 6.414\n","Epoch: 8 (Step: 002945): Train loss: 6.269, Val loss: 6.404\n","Epoch: 8 (Step: 002950): Train loss: 6.332, Val loss: 6.404\n","Epoch: 8 (Step: 002955): Train loss: 6.288, Val loss: 6.409\n","Epoch: 8 (Step: 002960): Train loss: 6.257, Val loss: 6.405\n","Epoch: 8 (Step: 002965): Train loss: 6.222, Val loss: 6.405\n","Epoch: 8 (Step: 002970): Train loss: 6.268, Val loss: 6.395\n","Epoch: 8 (Step: 002975): Train loss: 6.180, Val loss: 6.395\n","Epoch: 8 (Step: 002980): Train loss: 6.275, Val loss: 6.394\n","Epoch: 8 (Step: 002985): Train loss: 6.321, Val loss: 6.399\n","Epoch: 8 (Step: 002990): Train loss: 6.166, Val loss: 6.396\n","Epoch: 8 (Step: 002995): Train loss: 6.123, Val loss: 6.407\n","Epoch: 8 (Step: 003000): Train loss: 6.188, Val loss: 6.391\n","Epoch: 8 (Step: 003005): Train loss: 6.178, Val loss: 6.392\n","Epoch: 8 (Step: 003010): Train loss: 6.187, Val loss: 6.388\n","Epoch: 8 (Step: 003015): Train loss: 6.047, Val loss: 6.384\n","Epoch: 8 (Step: 003020): Train loss: 6.348, Val loss: 6.386\n","Epoch: 8 (Step: 003025): Train loss: 6.080, Val loss: 6.384\n","Epoch: 8 (Step: 003030): Train loss: 6.210, Val loss: 6.386\n","Epoch: 8 (Step: 003035): Train loss: 6.267, Val loss: 6.386\n","Epoch: 8 (Step: 003040): Train loss: 6.226, Val loss: 6.383\n","Epoch: 8 (Step: 003045): Train loss: 6.146, Val loss: 6.384\n","Epoch: 8 (Step: 003050): Train loss: 6.321, Val loss: 6.380\n","Epoch: 8 (Step: 003055): Train loss: 6.324, Val loss: 6.374\n","Epoch: 8 (Step: 003060): Train loss: 6.171, Val loss: 6.380\n","Epoch: 8 (Step: 003065): Train loss: 6.151, Val loss: 6.375\n","Epoch: 8 (Step: 003070): Train loss: 6.377, Val loss: 6.375\n","Epoch: 8 (Step: 003075): Train loss: 6.114, Val loss: 6.365\n","Epoch: 8 (Step: 003080): Train loss: 6.191, Val loss: 6.369\n","Epoch: 8 (Step: 003085): Train loss: 6.234, Val loss: 6.367\n","Epoch: 8 (Step: 003090): Train loss: 6.205, Val loss: 6.366\n","Epoch: 8 (Step: 003095): Train loss: 6.121, Val loss: 6.369\n","hami sabai bhanda dherai chan . tara , tara , tara , tara , hami , tara , tara , hami , hami , hami , hami , hami , hami , hami , hami , hami , hamro , hami , hami , hami , hami , hami , hami ,\n","Epoch: 9 (Step: 003100): Train loss: 6.197, Val loss: 6.365\n","Epoch: 9 (Step: 003105): Train loss: 6.142, Val loss: 6.358\n","Epoch: 9 (Step: 003110): Train loss: 6.051, Val loss: 6.360\n","Epoch: 9 (Step: 003115): Train loss: 6.067, Val loss: 6.361\n","Epoch: 9 (Step: 003120): Train loss: 6.017, Val loss: 6.361\n","Epoch: 9 (Step: 003125): Train loss: 6.190, Val loss: 6.355\n","Epoch: 9 (Step: 003130): Train loss: 6.276, Val loss: 6.357\n","Epoch: 9 (Step: 003135): Train loss: 6.212, Val loss: 6.364\n","Epoch: 9 (Step: 003140): Train loss: 6.205, Val loss: 6.354\n","Epoch: 9 (Step: 003145): Train loss: 6.076, Val loss: 6.357\n","Epoch: 9 (Step: 003150): Train loss: 6.171, Val loss: 6.354\n","Epoch: 9 (Step: 003155): Train loss: 6.118, Val loss: 6.359\n","Epoch: 9 (Step: 003160): Train loss: 6.040, Val loss: 6.357\n","Epoch: 9 (Step: 003165): Train loss: 6.162, Val loss: 6.353\n","Epoch: 9 (Step: 003170): Train loss: 6.086, Val loss: 6.357\n","Epoch: 9 (Step: 003175): Train loss: 6.133, Val loss: 6.353\n","Epoch: 9 (Step: 003180): Train loss: 6.075, Val loss: 6.350\n","Epoch: 9 (Step: 003185): Train loss: 6.063, Val loss: 6.348\n","Epoch: 9 (Step: 003190): Train loss: 6.219, Val loss: 6.346\n","Epoch: 9 (Step: 003195): Train loss: 6.101, Val loss: 6.348\n","Epoch: 9 (Step: 003200): Train loss: 6.131, Val loss: 6.347\n","Epoch: 9 (Step: 003205): Train loss: 6.249, Val loss: 6.340\n","Epoch: 9 (Step: 003210): Train loss: 6.146, Val loss: 6.342\n","Epoch: 9 (Step: 003215): Train loss: 6.095, Val loss: 6.339\n","Epoch: 9 (Step: 003220): Train loss: 6.164, Val loss: 6.362\n","Epoch: 9 (Step: 003225): Train loss: 6.246, Val loss: 6.336\n","Epoch: 9 (Step: 003230): Train loss: 6.217, Val loss: 6.340\n","Epoch: 9 (Step: 003235): Train loss: 6.136, Val loss: 6.339\n","Epoch: 9 (Step: 003240): Train loss: 6.098, Val loss: 6.341\n","Epoch: 9 (Step: 003245): Train loss: 6.291, Val loss: 6.336\n","Epoch: 9 (Step: 003250): Train loss: 6.162, Val loss: 6.330\n","Epoch: 9 (Step: 003255): Train loss: 6.155, Val loss: 6.337\n","Epoch: 9 (Step: 003260): Train loss: 6.136, Val loss: 6.333\n","Epoch: 9 (Step: 003265): Train loss: 6.104, Val loss: 6.334\n","Epoch: 9 (Step: 003270): Train loss: 6.046, Val loss: 6.332\n","Epoch: 9 (Step: 003275): Train loss: 6.109, Val loss: 6.331\n","Epoch: 9 (Step: 003280): Train loss: 6.003, Val loss: 6.326\n","Epoch: 9 (Step: 003285): Train loss: 6.063, Val loss: 6.324\n","Epoch: 9 (Step: 003290): Train loss: 6.144, Val loss: 6.328\n","Epoch: 9 (Step: 003295): Train loss: 5.977, Val loss: 6.323\n","Epoch: 9 (Step: 003300): Train loss: 6.106, Val loss: 6.325\n","Epoch: 9 (Step: 003305): Train loss: 6.082, Val loss: 6.320\n","Epoch: 9 (Step: 003310): Train loss: 6.040, Val loss: 6.320\n","Epoch: 9 (Step: 003315): Train loss: 6.096, Val loss: 6.322\n","Epoch: 9 (Step: 003320): Train loss: 6.171, Val loss: 6.316\n","Epoch: 9 (Step: 003325): Train loss: 6.203, Val loss: 6.312\n","Epoch: 9 (Step: 003330): Train loss: 6.157, Val loss: 6.320\n","Epoch: 9 (Step: 003335): Train loss: 6.094, Val loss: 6.322\n","Epoch: 9 (Step: 003340): Train loss: 6.156, Val loss: 6.323\n","Epoch: 9 (Step: 003345): Train loss: 5.992, Val loss: 6.308\n","Epoch: 9 (Step: 003350): Train loss: 6.063, Val loss: 6.316\n","Epoch: 9 (Step: 003355): Train loss: 6.213, Val loss: 6.306\n","Epoch: 9 (Step: 003360): Train loss: 5.943, Val loss: 6.306\n","Epoch: 9 (Step: 003365): Train loss: 6.201, Val loss: 6.307\n","Epoch: 9 (Step: 003370): Train loss: 6.236, Val loss: 6.304\n","Epoch: 9 (Step: 003375): Train loss: 6.058, Val loss: 6.304\n","Epoch: 9 (Step: 003380): Train loss: 6.124, Val loss: 6.303\n","Epoch: 9 (Step: 003385): Train loss: 6.095, Val loss: 6.297\n","Epoch: 9 (Step: 003390): Train loss: 6.107, Val loss: 6.299\n","Epoch: 9 (Step: 003395): Train loss: 6.056, Val loss: 6.296\n","Epoch: 9 (Step: 003400): Train loss: 6.078, Val loss: 6.307\n","Epoch: 9 (Step: 003405): Train loss: 6.074, Val loss: 6.310\n","Epoch: 9 (Step: 003410): Train loss: 6.041, Val loss: 6.304\n","Epoch: 9 (Step: 003415): Train loss: 6.131, Val loss: 6.296\n","Epoch: 9 (Step: 003420): Train loss: 5.966, Val loss: 6.293\n","Epoch: 9 (Step: 003425): Train loss: 6.231, Val loss: 6.289\n","Epoch: 9 (Step: 003430): Train loss: 6.077, Val loss: 6.287\n","Epoch: 9 (Step: 003435): Train loss: 6.024, Val loss: 6.297\n","hami sabai kura ho . tara , tara , tyo kura ho , tyo pani ho , tara , tara , tyo kura ho , tyo ta ? ke ho ? ke ho ? ke ho ? ke ho ? ke ho ? ke ho ? ke ho ? ke ho ?\n","Epoch: 10 (Step: 003440): Train loss: 5.815, Val loss: 6.287\n","Epoch: 10 (Step: 003445): Train loss: 6.092, Val loss: 6.283\n","Epoch: 10 (Step: 003450): Train loss: 5.935, Val loss: 6.292\n","Epoch: 10 (Step: 003455): Train loss: 6.004, Val loss: 6.292\n","Epoch: 10 (Step: 003460): Train loss: 5.972, Val loss: 6.291\n","Epoch: 10 (Step: 003465): Train loss: 6.110, Val loss: 6.282\n","Epoch: 10 (Step: 003470): Train loss: 6.110, Val loss: 6.285\n","Epoch: 10 (Step: 003475): Train loss: 5.997, Val loss: 6.287\n","Epoch: 10 (Step: 003480): Train loss: 6.009, Val loss: 6.289\n","Epoch: 10 (Step: 003485): Train loss: 6.173, Val loss: 6.289\n","Epoch: 10 (Step: 003490): Train loss: 5.996, Val loss: 6.286\n","Epoch: 10 (Step: 003495): Train loss: 5.966, Val loss: 6.284\n","Epoch: 10 (Step: 003500): Train loss: 5.957, Val loss: 6.280\n","Epoch: 10 (Step: 003505): Train loss: 6.006, Val loss: 6.281\n","Epoch: 10 (Step: 003510): Train loss: 5.966, Val loss: 6.282\n","Epoch: 10 (Step: 003515): Train loss: 6.017, Val loss: 6.276\n","Epoch: 10 (Step: 003520): Train loss: 6.091, Val loss: 6.287\n","Epoch: 10 (Step: 003525): Train loss: 6.022, Val loss: 6.276\n","Epoch: 10 (Step: 003530): Train loss: 6.073, Val loss: 6.272\n","Epoch: 10 (Step: 003535): Train loss: 6.088, Val loss: 6.275\n","Epoch: 10 (Step: 003540): Train loss: 5.939, Val loss: 6.266\n","Epoch: 10 (Step: 003545): Train loss: 6.040, Val loss: 6.270\n","Epoch: 10 (Step: 003550): Train loss: 6.095, Val loss: 6.269\n","Epoch: 10 (Step: 003555): Train loss: 6.172, Val loss: 6.280\n","Epoch: 10 (Step: 003560): Train loss: 6.126, Val loss: 6.266\n","Epoch: 10 (Step: 003565): Train loss: 5.959, Val loss: 6.264\n","Epoch: 10 (Step: 003570): Train loss: 6.022, Val loss: 6.267\n","Epoch: 10 (Step: 003575): Train loss: 6.082, Val loss: 6.268\n","Epoch: 10 (Step: 003580): Train loss: 6.122, Val loss: 6.263\n","Epoch: 10 (Step: 003585): Train loss: 5.982, Val loss: 6.264\n","Epoch: 10 (Step: 003590): Train loss: 6.064, Val loss: 6.267\n","Epoch: 10 (Step: 003595): Train loss: 6.045, Val loss: 6.267\n","Epoch: 10 (Step: 003600): Train loss: 5.933, Val loss: 6.257\n","Epoch: 10 (Step: 003605): Train loss: 5.935, Val loss: 6.263\n","Epoch: 10 (Step: 003610): Train loss: 6.184, Val loss: 6.257\n","Epoch: 10 (Step: 003615): Train loss: 5.933, Val loss: 6.263\n","Epoch: 10 (Step: 003620): Train loss: 5.921, Val loss: 6.259\n","Epoch: 10 (Step: 003625): Train loss: 6.133, Val loss: 6.260\n","Epoch: 10 (Step: 003630): Train loss: 5.995, Val loss: 6.255\n","Epoch: 10 (Step: 003635): Train loss: 6.075, Val loss: 6.254\n","Epoch: 10 (Step: 003640): Train loss: 6.039, Val loss: 6.247\n","Epoch: 10 (Step: 003645): Train loss: 6.037, Val loss: 6.249\n","Epoch: 10 (Step: 003650): Train loss: 5.880, Val loss: 6.250\n","Epoch: 10 (Step: 003655): Train loss: 5.928, Val loss: 6.249\n","Epoch: 10 (Step: 003660): Train loss: 5.911, Val loss: 6.249\n","Epoch: 10 (Step: 003665): Train loss: 6.013, Val loss: 6.244\n","Epoch: 10 (Step: 003670): Train loss: 5.992, Val loss: 6.246\n","Epoch: 10 (Step: 003675): Train loss: 5.955, Val loss: 6.240\n","Epoch: 10 (Step: 003680): Train loss: 6.036, Val loss: 6.240\n","Epoch: 10 (Step: 003685): Train loss: 5.945, Val loss: 6.246\n","Epoch: 10 (Step: 003690): Train loss: 6.037, Val loss: 6.249\n","Epoch: 10 (Step: 003695): Train loss: 5.940, Val loss: 6.234\n","Epoch: 10 (Step: 003700): Train loss: 5.928, Val loss: 6.236\n","Epoch: 10 (Step: 003705): Train loss: 6.151, Val loss: 6.229\n","Epoch: 10 (Step: 003710): Train loss: 6.070, Val loss: 6.235\n","Epoch: 10 (Step: 003715): Train loss: 5.997, Val loss: 6.238\n","Epoch: 10 (Step: 003720): Train loss: 6.064, Val loss: 6.233\n","Epoch: 10 (Step: 003725): Train loss: 6.169, Val loss: 6.225\n","Epoch: 10 (Step: 003730): Train loss: 5.885, Val loss: 6.229\n","Epoch: 10 (Step: 003735): Train loss: 6.043, Val loss: 6.223\n","Epoch: 10 (Step: 003740): Train loss: 5.956, Val loss: 6.239\n","Epoch: 10 (Step: 003745): Train loss: 6.065, Val loss: 6.232\n","Epoch: 10 (Step: 003750): Train loss: 6.127, Val loss: 6.225\n","Epoch: 10 (Step: 003755): Train loss: 6.106, Val loss: 6.225\n","Epoch: 10 (Step: 003760): Train loss: 5.950, Val loss: 6.229\n","Epoch: 10 (Step: 003765): Train loss: 6.019, Val loss: 6.216\n","Epoch: 10 (Step: 003770): Train loss: 6.056, Val loss: 6.223\n","Epoch: 10 (Step: 003775): Train loss: 5.990, Val loss: 6.221\n","Epoch: 10 (Step: 003780): Train loss: 5.952, Val loss: 6.228\n","hami sabai kura ho . tara , tara , tyo kura ho , tyo pani ho . tara , tara , tyo kura ho , tyo kura ho , tyo ho , tyo kura ho , tyo kura ho , tyo ho , tyo ta ? ke ho ? ke ho ?\n","Epoch: 11 (Step: 003785): Train loss: 5.906, Val loss: 6.223\n","Epoch: 11 (Step: 003790): Train loss: 5.870, Val loss: 6.220\n","Epoch: 11 (Step: 003795): Train loss: 5.873, Val loss: 6.224\n","Epoch: 11 (Step: 003800): Train loss: 5.834, Val loss: 6.219\n","Epoch: 11 (Step: 003805): Train loss: 5.912, Val loss: 6.219\n","Epoch: 11 (Step: 003810): Train loss: 5.828, Val loss: 6.228\n","Epoch: 11 (Step: 003815): Train loss: 5.949, Val loss: 6.211\n","Epoch: 11 (Step: 003820): Train loss: 5.919, Val loss: 6.217\n","Epoch: 11 (Step: 003825): Train loss: 5.843, Val loss: 6.211\n","Epoch: 11 (Step: 003830): Train loss: 5.936, Val loss: 6.218\n","Epoch: 11 (Step: 003835): Train loss: 5.927, Val loss: 6.217\n","Epoch: 11 (Step: 003840): Train loss: 5.972, Val loss: 6.219\n","Epoch: 11 (Step: 003845): Train loss: 5.835, Val loss: 6.216\n","Epoch: 11 (Step: 003850): Train loss: 5.848, Val loss: 6.212\n","Epoch: 11 (Step: 003855): Train loss: 5.968, Val loss: 6.212\n","Epoch: 11 (Step: 003860): Train loss: 5.858, Val loss: 6.207\n","Epoch: 11 (Step: 003865): Train loss: 5.981, Val loss: 6.202\n","Epoch: 11 (Step: 003870): Train loss: 5.889, Val loss: 6.204\n","Epoch: 11 (Step: 003875): Train loss: 5.871, Val loss: 6.201\n","Epoch: 11 (Step: 003880): Train loss: 5.952, Val loss: 6.209\n","Epoch: 11 (Step: 003885): Train loss: 6.003, Val loss: 6.208\n","Epoch: 11 (Step: 003890): Train loss: 5.891, Val loss: 6.203\n","Epoch: 11 (Step: 003895): Train loss: 5.996, Val loss: 6.200\n","Epoch: 11 (Step: 003900): Train loss: 5.990, Val loss: 6.199\n","Epoch: 11 (Step: 003905): Train loss: 5.975, Val loss: 6.203\n","Epoch: 11 (Step: 003910): Train loss: 5.864, Val loss: 6.198\n","Epoch: 11 (Step: 003915): Train loss: 6.011, Val loss: 6.204\n","Epoch: 11 (Step: 003920): Train loss: 6.014, Val loss: 6.201\n","Epoch: 11 (Step: 003925): Train loss: 6.007, Val loss: 6.196\n","Epoch: 11 (Step: 003930): Train loss: 5.990, Val loss: 6.197\n","Epoch: 11 (Step: 003935): Train loss: 5.809, Val loss: 6.196\n","Epoch: 11 (Step: 003940): Train loss: 5.904, Val loss: 6.201\n","Epoch: 11 (Step: 003945): Train loss: 5.903, Val loss: 6.201\n","Epoch: 11 (Step: 003950): Train loss: 5.884, Val loss: 6.191\n","Epoch: 11 (Step: 003955): Train loss: 5.896, Val loss: 6.190\n","Epoch: 11 (Step: 003960): Train loss: 5.935, Val loss: 6.191\n","Epoch: 11 (Step: 003965): Train loss: 5.908, Val loss: 6.199\n","Epoch: 11 (Step: 003970): Train loss: 5.950, Val loss: 6.186\n","Epoch: 11 (Step: 003975): Train loss: 5.856, Val loss: 6.189\n","Epoch: 11 (Step: 003980): Train loss: 5.819, Val loss: 6.192\n","Epoch: 11 (Step: 003985): Train loss: 6.028, Val loss: 6.191\n","Epoch: 11 (Step: 003990): Train loss: 5.903, Val loss: 6.188\n","Epoch: 11 (Step: 003995): Train loss: 5.815, Val loss: 6.185\n","Epoch: 11 (Step: 004000): Train loss: 5.963, Val loss: 6.177\n","Epoch: 11 (Step: 004005): Train loss: 5.908, Val loss: 6.177\n","Epoch: 11 (Step: 004010): Train loss: 6.216, Val loss: 6.178\n","Epoch: 11 (Step: 004015): Train loss: 6.071, Val loss: 6.174\n","Epoch: 11 (Step: 004020): Train loss: 5.897, Val loss: 6.176\n","Epoch: 11 (Step: 004025): Train loss: 5.981, Val loss: 6.179\n","Epoch: 11 (Step: 004030): Train loss: 5.988, Val loss: 6.178\n","Epoch: 11 (Step: 004035): Train loss: 5.926, Val loss: 6.175\n","Epoch: 11 (Step: 004040): Train loss: 5.864, Val loss: 6.177\n","Epoch: 11 (Step: 004045): Train loss: 5.840, Val loss: 6.179\n","Epoch: 11 (Step: 004050): Train loss: 5.749, Val loss: 6.174\n","Epoch: 11 (Step: 004055): Train loss: 5.858, Val loss: 6.171\n","Epoch: 11 (Step: 004060): Train loss: 5.824, Val loss: 6.172\n","Epoch: 11 (Step: 004065): Train loss: 5.828, Val loss: 6.174\n","Epoch: 11 (Step: 004070): Train loss: 5.895, Val loss: 6.168\n","Epoch: 11 (Step: 004075): Train loss: 5.833, Val loss: 6.164\n","Epoch: 11 (Step: 004080): Train loss: 6.002, Val loss: 6.176\n","Epoch: 11 (Step: 004085): Train loss: 5.985, Val loss: 6.172\n","Epoch: 11 (Step: 004090): Train loss: 5.938, Val loss: 6.165\n","Epoch: 11 (Step: 004095): Train loss: 5.797, Val loss: 6.172\n","Epoch: 11 (Step: 004100): Train loss: 5.925, Val loss: 6.160\n","Epoch: 11 (Step: 004105): Train loss: 5.925, Val loss: 6.158\n","Epoch: 11 (Step: 004110): Train loss: 5.976, Val loss: 6.163\n","Epoch: 11 (Step: 004115): Train loss: 5.727, Val loss: 6.160\n","Epoch: 11 (Step: 004120): Train loss: 5.980, Val loss: 6.161\n","Epoch: 11 (Step: 004125): Train loss: 5.853, Val loss: 6.159\n","hami sabai kura ho . tara , tara , tyo pani hoina , tyo pani ho . tara , tara , tyo pani ho , tyo pani ho , tyo pani ho . tara , tyo pani ho . tara , tyo pani ho , tyo pani , tyo pani ho .\n","Epoch: 12 (Step: 004130): Train loss: 5.912, Val loss: 6.161\n","Epoch: 12 (Step: 004135): Train loss: 5.710, Val loss: 6.160\n","Epoch: 12 (Step: 004140): Train loss: 5.828, Val loss: 6.161\n","Epoch: 12 (Step: 004145): Train loss: 5.874, Val loss: 6.162\n","Epoch: 12 (Step: 004150): Train loss: 6.021, Val loss: 6.157\n","Epoch: 12 (Step: 004155): Train loss: 5.868, Val loss: 6.158\n","Epoch: 12 (Step: 004160): Train loss: 5.804, Val loss: 6.156\n","Epoch: 12 (Step: 004165): Train loss: 5.867, Val loss: 6.158\n","Epoch: 12 (Step: 004170): Train loss: 5.870, Val loss: 6.156\n","Epoch: 12 (Step: 004175): Train loss: 5.785, Val loss: 6.163\n","Epoch: 12 (Step: 004180): Train loss: 5.897, Val loss: 6.157\n","Epoch: 12 (Step: 004185): Train loss: 5.840, Val loss: 6.159\n","Epoch: 12 (Step: 004190): Train loss: 5.883, Val loss: 6.154\n","Epoch: 12 (Step: 004195): Train loss: 5.980, Val loss: 6.146\n","Epoch: 12 (Step: 004200): Train loss: 5.777, Val loss: 6.155\n","Epoch: 12 (Step: 004205): Train loss: 5.853, Val loss: 6.154\n","Epoch: 12 (Step: 004210): Train loss: 5.772, Val loss: 6.149\n","Epoch: 12 (Step: 004215): Train loss: 5.848, Val loss: 6.145\n","Epoch: 12 (Step: 004220): Train loss: 5.808, Val loss: 6.150\n","Epoch: 12 (Step: 004225): Train loss: 5.848, Val loss: 6.148\n","Epoch: 12 (Step: 004230): Train loss: 5.936, Val loss: 6.148\n","Epoch: 12 (Step: 004235): Train loss: 5.899, Val loss: 6.145\n","Epoch: 12 (Step: 004240): Train loss: 5.932, Val loss: 6.150\n","Epoch: 12 (Step: 004245): Train loss: 5.893, Val loss: 6.145\n","Epoch: 12 (Step: 004250): Train loss: 5.882, Val loss: 6.143\n","Epoch: 12 (Step: 004255): Train loss: 5.846, Val loss: 6.141\n","Epoch: 12 (Step: 004260): Train loss: 5.816, Val loss: 6.148\n","Epoch: 12 (Step: 004265): Train loss: 5.980, Val loss: 6.139\n","Epoch: 12 (Step: 004270): Train loss: 5.847, Val loss: 6.138\n","Epoch: 12 (Step: 004275): Train loss: 5.918, Val loss: 6.130\n","Epoch: 12 (Step: 004280): Train loss: 5.949, Val loss: 6.137\n","Epoch: 12 (Step: 004285): Train loss: 5.908, Val loss: 6.143\n","Epoch: 12 (Step: 004290): Train loss: 5.933, Val loss: 6.142\n","Epoch: 12 (Step: 004295): Train loss: 5.867, Val loss: 6.138\n","Epoch: 12 (Step: 004300): Train loss: 5.851, Val loss: 6.130\n","Epoch: 12 (Step: 004305): Train loss: 5.813, Val loss: 6.134\n","Epoch: 12 (Step: 004310): Train loss: 5.738, Val loss: 6.136\n","Epoch: 12 (Step: 004315): Train loss: 5.704, Val loss: 6.133\n","Epoch: 12 (Step: 004320): Train loss: 5.918, Val loss: 6.128\n","Epoch: 12 (Step: 004325): Train loss: 5.794, Val loss: 6.136\n","Epoch: 12 (Step: 004330): Train loss: 5.791, Val loss: 6.124\n","Epoch: 12 (Step: 004335): Train loss: 5.774, Val loss: 6.129\n","Epoch: 12 (Step: 004340): Train loss: 5.968, Val loss: 6.128\n","Epoch: 12 (Step: 004345): Train loss: 5.860, Val loss: 6.135\n","Epoch: 12 (Step: 004350): Train loss: 5.809, Val loss: 6.126\n","Epoch: 12 (Step: 004355): Train loss: 5.654, Val loss: 6.129\n","Epoch: 12 (Step: 004360): Train loss: 5.863, Val loss: 6.126\n","Epoch: 12 (Step: 004365): Train loss: 5.725, Val loss: 6.121\n","Epoch: 12 (Step: 004370): Train loss: 5.911, Val loss: 6.130\n","Epoch: 12 (Step: 004375): Train loss: 5.855, Val loss: 6.129\n","Epoch: 12 (Step: 004380): Train loss: 5.797, Val loss: 6.125\n","Epoch: 12 (Step: 004385): Train loss: 5.924, Val loss: 6.122\n","Epoch: 12 (Step: 004390): Train loss: 5.930, Val loss: 6.121\n","Epoch: 12 (Step: 004395): Train loss: 5.731, Val loss: 6.120\n","Epoch: 12 (Step: 004400): Train loss: 5.798, Val loss: 6.121\n","Epoch: 12 (Step: 004405): Train loss: 5.797, Val loss: 6.119\n","Epoch: 12 (Step: 004410): Train loss: 5.770, Val loss: 6.122\n","Epoch: 12 (Step: 004415): Train loss: 5.804, Val loss: 6.119\n","Epoch: 12 (Step: 004420): Train loss: 5.770, Val loss: 6.117\n","Epoch: 12 (Step: 004425): Train loss: 5.812, Val loss: 6.115\n","Epoch: 12 (Step: 004430): Train loss: 5.809, Val loss: 6.117\n","Epoch: 12 (Step: 004435): Train loss: 5.791, Val loss: 6.112\n","Epoch: 12 (Step: 004440): Train loss: 5.735, Val loss: 6.104\n","Epoch: 12 (Step: 004445): Train loss: 5.771, Val loss: 6.110\n","Epoch: 12 (Step: 004450): Train loss: 5.857, Val loss: 6.111\n","Epoch: 12 (Step: 004455): Train loss: 5.790, Val loss: 6.107\n","Epoch: 12 (Step: 004460): Train loss: 5.890, Val loss: 6.107\n","Epoch: 12 (Step: 004465): Train loss: 5.751, Val loss: 6.107\n","Epoch: 12 (Step: 004470): Train loss: 5.902, Val loss: 6.109\n","hami sabai bhanda dherai chan . tara , yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho . yo kura ho .\n","Epoch: 13 (Step: 004475): Train loss: 5.812, Val loss: 6.102\n","Epoch: 13 (Step: 004480): Train loss: 5.816, Val loss: 6.108\n","Epoch: 13 (Step: 004485): Train loss: 5.811, Val loss: 6.102\n","Epoch: 13 (Step: 004490): Train loss: 5.845, Val loss: 6.117\n","Epoch: 13 (Step: 004495): Train loss: 5.706, Val loss: 6.105\n","Epoch: 13 (Step: 004500): Train loss: 5.626, Val loss: 6.108\n","Epoch: 13 (Step: 004505): Train loss: 5.723, Val loss: 6.109\n","Epoch: 13 (Step: 004510): Train loss: 5.733, Val loss: 6.102\n","Epoch: 13 (Step: 004515): Train loss: 5.646, Val loss: 6.100\n","Epoch: 13 (Step: 004520): Train loss: 5.773, Val loss: 6.106\n","Epoch: 13 (Step: 004525): Train loss: 5.771, Val loss: 6.103\n","Epoch: 13 (Step: 004530): Train loss: 5.659, Val loss: 6.099\n","Epoch: 13 (Step: 004535): Train loss: 5.663, Val loss: 6.099\n","Epoch: 13 (Step: 004540): Train loss: 5.870, Val loss: 6.099\n","Epoch: 13 (Step: 004545): Train loss: 5.803, Val loss: 6.104\n","Epoch: 13 (Step: 004550): Train loss: 5.735, Val loss: 6.104\n","Epoch: 13 (Step: 004555): Train loss: 5.688, Val loss: 6.109\n","Epoch: 13 (Step: 004560): Train loss: 5.704, Val loss: 6.095\n","Epoch: 13 (Step: 004565): Train loss: 5.841, Val loss: 6.093\n","Epoch: 13 (Step: 004570): Train loss: 5.594, Val loss: 6.092\n","Epoch: 13 (Step: 004575): Train loss: 5.690, Val loss: 6.095\n","Epoch: 13 (Step: 004580): Train loss: 5.850, Val loss: 6.100\n","Epoch: 13 (Step: 004585): Train loss: 5.706, Val loss: 6.094\n","Epoch: 13 (Step: 004590): Train loss: 5.658, Val loss: 6.089\n","Epoch: 13 (Step: 004595): Train loss: 5.645, Val loss: 6.091\n","Epoch: 13 (Step: 004600): Train loss: 5.857, Val loss: 6.089\n","Epoch: 13 (Step: 004605): Train loss: 5.716, Val loss: 6.097\n","Epoch: 13 (Step: 004610): Train loss: 5.803, Val loss: 6.094\n","Epoch: 13 (Step: 004615): Train loss: 5.915, Val loss: 6.087\n","Epoch: 13 (Step: 004620): Train loss: 5.737, Val loss: 6.088\n","Epoch: 13 (Step: 004625): Train loss: 5.782, Val loss: 6.084\n","Epoch: 13 (Step: 004630): Train loss: 5.715, Val loss: 6.087\n","Epoch: 13 (Step: 004635): Train loss: 5.792, Val loss: 6.079\n","Epoch: 13 (Step: 004640): Train loss: 5.810, Val loss: 6.080\n","Epoch: 13 (Step: 004645): Train loss: 5.736, Val loss: 6.091\n","Epoch: 13 (Step: 004650): Train loss: 5.749, Val loss: 6.089\n","Epoch: 13 (Step: 004655): Train loss: 5.753, Val loss: 6.085\n","Epoch: 13 (Step: 004660): Train loss: 5.701, Val loss: 6.085\n","Epoch: 13 (Step: 004665): Train loss: 5.857, Val loss: 6.080\n","Epoch: 13 (Step: 004670): Train loss: 5.723, Val loss: 6.081\n","Epoch: 13 (Step: 004675): Train loss: 5.875, Val loss: 6.085\n","Epoch: 13 (Step: 004680): Train loss: 5.665, Val loss: 6.077\n","Epoch: 13 (Step: 004685): Train loss: 5.664, Val loss: 6.082\n","Epoch: 13 (Step: 004690): Train loss: 5.726, Val loss: 6.078\n","Epoch: 13 (Step: 004695): Train loss: 5.669, Val loss: 6.076\n","Epoch: 13 (Step: 004700): Train loss: 5.722, Val loss: 6.075\n","Epoch: 13 (Step: 004705): Train loss: 5.913, Val loss: 6.076\n","Epoch: 13 (Step: 004710): Train loss: 5.788, Val loss: 6.084\n","Epoch: 13 (Step: 004715): Train loss: 5.748, Val loss: 6.072\n","Epoch: 13 (Step: 004720): Train loss: 5.861, Val loss: 6.078\n","Epoch: 13 (Step: 004725): Train loss: 5.727, Val loss: 6.073\n","Epoch: 13 (Step: 004730): Train loss: 5.771, Val loss: 6.076\n","Epoch: 13 (Step: 004735): Train loss: 5.869, Val loss: 6.068\n","Epoch: 13 (Step: 004740): Train loss: 5.814, Val loss: 6.079\n","Epoch: 13 (Step: 004745): Train loss: 5.732, Val loss: 6.066\n","Epoch: 13 (Step: 004750): Train loss: 5.737, Val loss: 6.067\n","Epoch: 13 (Step: 004755): Train loss: 5.518, Val loss: 6.077\n","Epoch: 13 (Step: 004760): Train loss: 5.600, Val loss: 6.072\n","Epoch: 13 (Step: 004765): Train loss: 5.713, Val loss: 6.068\n","Epoch: 13 (Step: 004770): Train loss: 5.862, Val loss: 6.069\n","Epoch: 13 (Step: 004775): Train loss: 5.845, Val loss: 6.065\n","Epoch: 13 (Step: 004780): Train loss: 5.820, Val loss: 6.065\n","Epoch: 13 (Step: 004785): Train loss: 5.805, Val loss: 6.058\n","Epoch: 13 (Step: 004790): Train loss: 5.644, Val loss: 6.062\n","Epoch: 13 (Step: 004795): Train loss: 5.783, Val loss: 6.071\n","Epoch: 13 (Step: 004800): Train loss: 5.748, Val loss: 6.061\n","Epoch: 13 (Step: 004805): Train loss: 5.838, Val loss: 6.054\n","Epoch: 13 (Step: 004810): Train loss: 5.800, Val loss: 6.056\n","Epoch: 13 (Step: 004815): Train loss: 5.601, Val loss: 6.058\n","hami sabai kura ho . tara , tara , tyo pani hoina , tyo pani ho . tara , tara , tyo pani ho , tyo pani ho , tyo ho , tyo ho , tyo ho , tyo ta , tyo ho , tyo ho , tyo ho , tyo ho\n","Epoch: 14 (Step: 004820): Train loss: 5.526, Val loss: 6.062\n","Epoch: 14 (Step: 004825): Train loss: 5.433, Val loss: 6.061\n","Epoch: 14 (Step: 004830): Train loss: 5.597, Val loss: 6.054\n","Epoch: 14 (Step: 004835): Train loss: 5.692, Val loss: 6.054\n","Epoch: 14 (Step: 004840): Train loss: 5.588, Val loss: 6.050\n","Epoch: 14 (Step: 004845): Train loss: 5.620, Val loss: 6.060\n","Epoch: 14 (Step: 004850): Train loss: 5.718, Val loss: 6.053\n","Epoch: 14 (Step: 004855): Train loss: 5.669, Val loss: 6.057\n","Epoch: 14 (Step: 004860): Train loss: 5.679, Val loss: 6.056\n","Epoch: 14 (Step: 004865): Train loss: 5.737, Val loss: 6.053\n","Epoch: 14 (Step: 004870): Train loss: 5.641, Val loss: 6.051\n","Epoch: 14 (Step: 004875): Train loss: 5.767, Val loss: 6.056\n","Epoch: 14 (Step: 004880): Train loss: 5.741, Val loss: 6.047\n","Epoch: 14 (Step: 004885): Train loss: 5.829, Val loss: 6.056\n","Epoch: 14 (Step: 004890): Train loss: 5.656, Val loss: 6.053\n","Epoch: 14 (Step: 004895): Train loss: 5.507, Val loss: 6.055\n","Epoch: 14 (Step: 004900): Train loss: 5.780, Val loss: 6.048\n","Epoch: 14 (Step: 004905): Train loss: 5.705, Val loss: 6.046\n","Epoch: 14 (Step: 004910): Train loss: 5.684, Val loss: 6.050\n","Epoch: 14 (Step: 004915): Train loss: 5.681, Val loss: 6.049\n","Epoch: 14 (Step: 004920): Train loss: 5.683, Val loss: 6.051\n","Epoch: 14 (Step: 004925): Train loss: 5.711, Val loss: 6.043\n","Epoch: 14 (Step: 004930): Train loss: 5.589, Val loss: 6.047\n","Epoch: 14 (Step: 004935): Train loss: 5.646, Val loss: 6.047\n","Epoch: 14 (Step: 004940): Train loss: 5.715, Val loss: 6.045\n","Epoch: 14 (Step: 004945): Train loss: 5.614, Val loss: 6.049\n","Epoch: 14 (Step: 004950): Train loss: 5.740, Val loss: 6.050\n","Epoch: 14 (Step: 004955): Train loss: 5.642, Val loss: 6.052\n","Epoch: 14 (Step: 004960): Train loss: 5.707, Val loss: 6.043\n","Epoch: 14 (Step: 004965): Train loss: 5.547, Val loss: 6.043\n","Epoch: 14 (Step: 004970): Train loss: 5.578, Val loss: 6.040\n","Epoch: 14 (Step: 004975): Train loss: 5.563, Val loss: 6.041\n","Epoch: 14 (Step: 004980): Train loss: 5.598, Val loss: 6.038\n","Epoch: 14 (Step: 004985): Train loss: 5.659, Val loss: 6.043\n","Epoch: 14 (Step: 004990): Train loss: 5.848, Val loss: 6.041\n","Epoch: 14 (Step: 004995): Train loss: 5.761, Val loss: 6.042\n","Epoch: 14 (Step: 005000): Train loss: 5.622, Val loss: 6.032\n","Epoch: 14 (Step: 005005): Train loss: 5.613, Val loss: 6.037\n","Epoch: 14 (Step: 005010): Train loss: 5.687, Val loss: 6.033\n","Epoch: 14 (Step: 005015): Train loss: 5.561, Val loss: 6.044\n","Epoch: 14 (Step: 005020): Train loss: 5.557, Val loss: 6.029\n","Epoch: 14 (Step: 005025): Train loss: 5.636, Val loss: 6.035\n","Epoch: 14 (Step: 005030): Train loss: 5.672, Val loss: 6.032\n","Epoch: 14 (Step: 005035): Train loss: 5.602, Val loss: 6.032\n","Epoch: 14 (Step: 005040): Train loss: 5.650, Val loss: 6.033\n","Epoch: 14 (Step: 005045): Train loss: 5.649, Val loss: 6.023\n","Epoch: 14 (Step: 005050): Train loss: 5.534, Val loss: 6.030\n","Epoch: 14 (Step: 005055): Train loss: 5.635, Val loss: 6.026\n","Epoch: 14 (Step: 005060): Train loss: 5.670, Val loss: 6.026\n","Epoch: 14 (Step: 005065): Train loss: 5.642, Val loss: 6.032\n","Epoch: 14 (Step: 005070): Train loss: 5.705, Val loss: 6.027\n","Epoch: 14 (Step: 005075): Train loss: 5.716, Val loss: 6.026\n","Epoch: 14 (Step: 005080): Train loss: 5.554, Val loss: 6.033\n","Epoch: 14 (Step: 005085): Train loss: 5.614, Val loss: 6.022\n","Epoch: 14 (Step: 005090): Train loss: 5.722, Val loss: 6.025\n","Epoch: 14 (Step: 005095): Train loss: 5.535, Val loss: 6.023\n","Epoch: 14 (Step: 005100): Train loss: 5.739, Val loss: 6.024\n","Epoch: 14 (Step: 005105): Train loss: 5.714, Val loss: 6.016\n","Epoch: 14 (Step: 005110): Train loss: 5.601, Val loss: 6.016\n","Epoch: 14 (Step: 005115): Train loss: 5.714, Val loss: 6.022\n","Epoch: 14 (Step: 005120): Train loss: 5.585, Val loss: 6.017\n","Epoch: 14 (Step: 005125): Train loss: 5.578, Val loss: 6.013\n","Epoch: 14 (Step: 005130): Train loss: 5.678, Val loss: 6.017\n","Epoch: 14 (Step: 005135): Train loss: 5.506, Val loss: 6.015\n","Epoch: 14 (Step: 005140): Train loss: 5.740, Val loss: 6.019\n","Epoch: 14 (Step: 005145): Train loss: 5.620, Val loss: 6.016\n","Epoch: 14 (Step: 005150): Train loss: 5.766, Val loss: 6.013\n","Epoch: 14 (Step: 005155): Train loss: 5.706, Val loss: 6.017\n","hami sabai kura ho . tara , tara , tyo kura ta ? ke ho ? ke ke ke ? ke ke ke ? ke ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke ? ke\n","Training completed in 139.80 minutes.\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","\n","def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n","  fig, ax1 = plt.subplots(figsize=(5, 3))\n","\n","  ax1.plot(epochs_seen, train_losses, label=\"training loss\")\n","  ax1.plot(epochs_seen, val_losses, label=\"validation loss\")\n","  ax1.set_xlabel(\"Tokens seen\")\n","  ax1.set_ylabel(\"loss\")\n","  ax1.legend()\n","  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n","\n","  ax2 = ax1.twiny()\n","  ax2.plot(tokens_seen, train_losses, alpha=0)\n","  ax2.set_xlabel(\"Tokens seen\")\n","\n","  fig.tight_layout()\n","  plt.savefig(\"loss-plot.pdf\")\n","  plt.show()\n","\n","epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n","plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"dj0eVN5mtbpg","executionInfo":{"status":"ok","timestamp":1734943574662,"user_tz":-345,"elapsed":994,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"b18902e3-fcb7-4403-f4fb-821e46057008"},"execution_count":127,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x300 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## Decoding strategies to control randomness"],"metadata":{"id":"OXs3zxrQu6mh"}},{"cell_type":"code","source":["model.to(\"cpu\")\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDenLXsnu9sc","executionInfo":{"status":"ok","timestamp":1734943575315,"user_tz":-345,"elapsed":658,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"974a2295-898c-42c8-cd8e-f39211aafdaa"},"execution_count":128,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (tok_emb): Embedding(30000, 512)\n","  (pos_emb): Embedding(512, 512)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNormalization()\n","  (out_head): Linear(in_features=512, out_features=30000, bias=False)\n",")"]},"metadata":{},"execution_count":128}]},{"cell_type":"code","source":["token_ids = generate_text_sample(model, text_to_token_ids(\"Every effort moves you\", tokenizer), 100, GPT_CONFIG_124M[\"context_length\"])\n","print(token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7f1KkfxYvtak","executionInfo":{"status":"ok","timestamp":1734943582877,"user_tz":-345,"elapsed":7563,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"bc3f69d7-4c44-4eb3-edd7-7cbd740932cf"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["very e f for t mo ves you can in pharme e mail ) le gareko cha . kathamadaum . nepala rashtra baimkale ru . 1 arba rupaiya . n barabarako sheyara nishkasana gareko cha . baimkale ru . 1 arba rupaiya . n barabarako sheyara nishkasana gareko cha . baimkale ru . 1 arba rupaiya . n barabarako sheyara nishkasana gareko cha . baimkale ru . 1 arba rupaiya . n barabarako sheyara nishkasana gareko cha . baimkale ru . 1 arba rupaiya . n napha kamaeko cha . baimkale ru . n barabarako seyara karobara gareko cha . baimkale ru . 1 arba rupaiya . n\n"]}]},{"cell_type":"markdown","source":["## Temperature Scaling"],"metadata":{"id":"r2y0EyJnwiWO"}},{"cell_type":"code","source":["# def print_sampled_tokens(probas):\n","#   torch.manual_seed(123)\n","#   sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n","#   sampled_ids = torch.bincount(torch.tensor(sample))\n","\n","#   print(f\"Count of sampled token id: {sampled_ids}\")\n","\n","#   for i, freq in enumerate(sampled_ids):\n","#     print(f\"{freq} * {inverse_vocab[i]}\")\n","\n","# print_sampled_tokens(probas)"],"metadata":{"id":"bf7w0WAdxjce","executionInfo":{"status":"ok","timestamp":1734943582877,"user_tz":-345,"elapsed":4,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["def softmax_with_temperature(logits, temperature):\n","  scaled_logits = logits / temperature\n","  return torch.softmax(scaled_logits, dim=0)"],"metadata":{"id":"tTfOL8ZEzETW","executionInfo":{"status":"ok","timestamp":1734943582877,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":["Temperature greater than 1 will result in more uniformly distributed token probabilities after applying the softmax and the Temperature smaller than 1 will result in more confident distributions after applying hte softmax."],"metadata":{"id":"YxhaxegA0HF9"}},{"cell_type":"markdown","source":["## Top-K Sampling"],"metadata":{"id":"XwJD-oqz0ekN"}},{"cell_type":"markdown","source":["## Generate Text with Control Randomness"],"metadata":{"id":"CNahaLkr3xHi"}},{"cell_type":"code","source":["def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","  for _ in range(max_new_tokens):\n","    idx_cond = idx[:, -context_size:]\n","    with torch.no_grad():\n","      logits = model(idx_cond)\n","\n","    logits = logits[:, -1, :]\n","\n","    if top_k is not None:\n","      top_logits, _ = torch.topk(logits, top_k)\n","      # get the minimum value from the logits for all batch here (only one batch)\n","      min_val = top_logits[:, -1]\n","      logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")), logits).to(logits.device)\n","\n","    if temperature > 0.0:\n","      logits = logits / temperature\n","\n","      probs = torch.softmax(logits, dim=-1)\n","\n","      idx_next = torch.multinomial(probs, num_samples=1)\n","\n","    else:\n","      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n","\n","    if idx_next == eos_id:\n","      break\n","\n","    idx = torch.cat((idx, idx_next), dim=1) # (batch_size, num_tokens + 1)\n","\n","  return idx"],"metadata":{"id":"O7k9sa0-304U","executionInfo":{"status":"ok","timestamp":1734943582877,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["token_ids = generate(\n","    model=model,\n","    idx=text_to_token_ids(\"hello\", tokenizer),\n","    max_new_tokens=100,\n","    context_size=GPT_CONFIG_124M[\"context_length\"],\n","    top_k=None,\n","    temperature=1.2\n",")\n","\n","print(\"Output text: \", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kX422zfA4vMJ","executionInfo":{"status":"ok","timestamp":1734943642697,"user_tz":-345,"elapsed":7838,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"08553ba2-5102-4698-fbfb-fb5fe274ca84"},"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:  hello dai era unale mageko thiyo , jasari sathiharusa . nga pani bhanna sakchu , uni aisakeko layama thiyo , .' sarubhakta chaum . bechda antima jita pachi 11 ra jita vyavasaya garna sakdainan nishchita 2 kamsya saki . ndain . dhamala sambedanashila denta duvai tipsa sa . nga gare bamojima rojne ra yuropama jhagada maula une ani tyasapachi premale pariksha dina pachi gorkhalyanda batai gareko arko mudda dinchin . lade garna rcha . ghadi rahin . sa / pathegharako ayojanaharuko babu lagna sakenan jasta vyaktiharule 10 lakha madhye parivaralai kula prayogama lyauna sakcha bhanne sandesha tire bajetama pathyakrama hunuparcha bhanne\n"]}]},{"cell_type":"code","source":["token_ids = generate(\n","    model=model,\n","    idx=text_to_token_ids(\"tara j bane pani auta kura chai\", tokenizer),\n","    max_new_tokens=100,\n","    context_size=GPT_CONFIG_124M[\"context_length\"],\n","    top_k=None,\n","    temperature=1.2\n",")\n","\n","print(\"Output text: \", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQOEumiltD1Q","executionInfo":{"status":"ok","timestamp":1734944124515,"user_tz":-345,"elapsed":8762,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"fe3f3183-0a56-46ff-9a28-2de8f28d22b6"},"execution_count":142,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:  tara j bane pani auta kura chai biela lai harae . tatkalina samvadako lata ji chepa ~ naharu roshana ramabahadura guru ye naharule chuttairajya ta kla nagarne , suvasa yogi raja koirala , pratibha guru shanti samjhautaka hu nepaliko manyataka shraddha ul aphni se sam kau pi din di chauda sa purni yuga mandira hum riya shilalekha radhakrrishna gaya lai shahida kyamera samata ko puja garne anurodha gari jema kira svarupa paipalaina karyakrama kala bhavana vibhagaka pra janata sam palika ayojaka dahala ra devi re lalitapurako kala bi pi varama guru shu bahirieka thie vi sthapitale gai rva chanama sammelanako avasara rahema tiko vyaktitvako nidhanapachi hareka kshamata\n"]}]},{"cell_type":"markdown","source":["## Loading and saving model weights in PyTorch"],"metadata":{"id":"EjaRJNVJ53gc"}},{"cell_type":"code","source":["torch.save(model.state_dict(), \"model.pth\")"],"metadata":{"id":"xoym-9Ch59cz","executionInfo":{"status":"ok","timestamp":1734943590834,"user_tz":-345,"elapsed":2,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["model = GPTModel(GPT_CONFIG_124M)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.load_state_dict(torch.load(\"./model.pth\", map_location=device, weights_only=True))\n","\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4tk2Y-aY6CNK","executionInfo":{"status":"ok","timestamp":1734943591754,"user_tz":-345,"elapsed":922,"user":{"displayName":"Manoj Baniya","userId":"07800477237160464018"}},"outputId":"ca7f2056-9d9b-406d-8758-b972f9281f73"},"execution_count":135,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (tok_emb): Embedding(30000, 512)\n","  (pos_emb): Embedding(512, 512)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_Query): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Key): Linear(in_features=512, out_features=512, bias=False)\n","        (W_Value): Linear(in_features=512, out_features=512, bias=False)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GeLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","      )\n","      (norm1): LayerNormalization()\n","      (norm2): LayerNormalization()\n","      (drop_skip): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNormalization()\n","  (out_head): Linear(in_features=512, out_features=30000, bias=False)\n",")"]},"metadata":{},"execution_count":135}]},{"cell_type":"markdown","source":["It's common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD\n","These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later:"],"metadata":{"id":"KWMu7hWb6gbn"}}]}